import os
import yaml
from typing import List, Dict, Any, Tuple
import numpy as np
import torch
from FlagEmbedding import BGEM3FlagModel
import logging
from pathlib import Path
from utils.path_utils import get_config_path, get_model_cache_dir

logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (EXE í™˜ê²½ í˜¸í™˜)
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent.parent.parent

class BGEM3Embedder:
    """BGE-M3 ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë” (dense + sparse ë²¡í„°)"""
    
    def __init__(self, model_name: str = "BAAI/bge-m3", device: str = "cuda", config_path: str = "configs.yaml"):
        """
        BGE-M3 ì„ë² ë” ì´ˆê¸°í™”
        
        Args:
            model_name: Hugging Face ëª¨ë¸ ì´ë¦„
            device: 'cuda' ë˜ëŠ” 'cpu'
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        # CPU ëª¨ë“œë¡œ ê³ ì • (CUDA ì˜ì¡´ì„± ì œê±°)
        self.device = "cpu"
        logger.info("ğŸ“Œ CPU ëª¨ë“œë¡œ ì„ë² ë”©ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        self.config = self._load_config(config_path)
        self.dense_dim = self.config.get('embedding', {}).get('dense_dim', 1024)
        self.batch_size = self.config.get('embedding', {}).get('batch_size', 12)
        
        self.model_name = model_name
        self.model = None
        self._load_model()
    
    def _load_config(self, config_path: str) -> dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ (EXE í™˜ê²½ í˜¸í™˜)"""
        try:
            config_file_path = get_config_path(config_path)
            
            if config_file_path.exists():
                with open(config_file_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    logger.info(f"BGE-M3 ì„¤ì • íŒŒì¼ ë¡œë“œ ì„±ê³µ: {config_file_path}")
                    return config
            else:
                logger.warning(f"BGE-M3 ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_file_path}")
                return {}
                
        except Exception as e:
            logger.error(f"BGE-M3 ì„¤ì • ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {}

    def _load_model(self):
        """BGE-M3 ëª¨ë¸ ë¡œë“œ (ìµœì í™”ëœ ë²„ì „)"""
        try:
            logger.info(f"'{self.model_name}' ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if self.model is not None:
                logger.info("BGE-M3 ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                return
            
            # BGE-M3 ëª¨ë¸ ë¡œë“œ
            # use_fp16=True for faster inference
            self.model = BGEM3FlagModel(
                self.model_name,
                use_fp16=True if self.device == "cuda" else False,
                device=self.device
            )
            
            logger.info("âœ… BGE-M3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"BGE-M3 ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            logger.warning("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.model = None
    
    def encode_queries(self, queries: List[str], batch_size: int = None) -> Dict[str, np.ndarray]:
        """
        ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”© (ê²€ìƒ‰ìš©)
        
        Args:
            queries: ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
        
        Returns:
            {
                'dense_vecs': np.ndarray shape (len(queries), 1024),
                'sparse_vecs': List[Dict] sparse vectors
            }
        """
        if self.model is None:
            logger.error("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise RuntimeError("BGE-M3 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if batch_size is None:
            batch_size = self.batch_size
        
        try:
            # BGE-M3ì˜ encode ë©”ì„œë“œ ì‚¬ìš© (queryìš©)
            embeddings = self.model.encode(
                queries,
                batch_size=batch_size,
                max_length=8192,  # BGE-M3ëŠ” ìµœëŒ€ 8192 í† í° ì§€ì›
                return_dense=True,
                return_sparse=True,
                return_colbert_vecs=False  # ColBERTëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            )
            
            return {
                'dense_vecs': embeddings['dense_vecs'],  # (batch_size, 1024)
                'lexical_weights': embeddings['lexical_weights']  # sparse vectors
            }
            
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
            raise RuntimeError(f"ì¿¼ë¦¬ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
    
    def encode_documents(self, documents: List[str], batch_size: int = None) -> Dict[str, np.ndarray]:
        """
        ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”© (ì¸ë±ì‹±ìš©)
        
        Args:
            documents: ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
        
        Returns:
            {
                'dense_vecs': np.ndarray shape (len(documents), 1024),
                'sparse_vecs': List[Dict] sparse vectors
            }
        """
        if self.model is None:
            logger.error("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise RuntimeError("BGE-M3 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if batch_size is None:
            batch_size = self.batch_size
        
        try:
            # BGE-M3ì˜ encode ë©”ì„œë“œ ì‚¬ìš© (documentìš©)
            embeddings = self.model.encode(
                documents,
                batch_size=batch_size,
                max_length=8192,
                return_dense=True,
                return_sparse=True,
                return_colbert_vecs=False
            )
            
            return {
                'dense_vecs': embeddings['dense_vecs'],  # (batch_size, 1024)
                'lexical_weights': embeddings['lexical_weights']  # sparse vectors
            }
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
            raise RuntimeError(f"ë¬¸ì„œ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
    
    def encode_single_query(self, query: str) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ ì¸ì½”ë”©
        
        Args:
            query: ì¿¼ë¦¬ í…ìŠ¤íŠ¸
        
        Returns:
            {
                'dense_vec': np.ndarray shape (1024,),
                'sparse_vec': Dict sparse vector
            }
        """
        result = self.encode_queries([query])
        return {
            'dense_vec': result['dense_vecs'][0],
            'sparse_vec': result['lexical_weights'][0] if result['lexical_weights'] else {}
        }
    
    def get_embedding_dim(self) -> int:
        """Dense ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.dense_dim
    
    def convert_sparse_to_qdrant_format(self, sparse_dict: Dict[int, float]) -> Dict[str, Any]:
        """
        BGE-M3 sparse ë²¡í„°ë¥¼ Qdrant í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            sparse_dict: {token_id: weight, ...}
        
        Returns:
            {
                'indices': List[int],
                'values': List[float]
            }
        """
        if not sparse_dict:
            return {'indices': [], 'values': []}
        
        indices = list(sparse_dict.keys())
        values = list(sparse_dict.values())
        
        return {
            'indices': indices,
            'values': values
        }

