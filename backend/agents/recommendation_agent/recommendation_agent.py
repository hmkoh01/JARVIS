import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from collections import Counter
import logging

from ..base_agent import BaseAgent, AgentResponse
from database.sqlite_meta import SQLiteMeta  # ë³€ê²½ë¨: SQLAlchemy ëŒ€ì‹  SQLiteMeta ì‚¬ìš©
from config.settings import settings

logger = logging.getLogger(__name__)

class RecommendationAgent(BaseAgent):
    """ì¶”ì²œ ë° ì œì•ˆ ê´€ë ¨ ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__(
            agent_type="recommendation",
            description="ì¶”ì²œ, ì œì•ˆ, ì¶”ì²œí•´ì¤˜ ë“±ì˜ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."
        )
        self.sqlite_meta = SQLiteMeta()  # SQLite ë©”íƒ€ë°ì´í„° ì ‘ê·¼
    
    def process(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒíƒœë¥¼ ë°›ì•„ì„œ ì²˜ë¦¬í•˜ê³  ìˆ˜ì •ëœ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        question = state.get("question", "")
        user_id = state.get("user_id")
        
        if not question:
            return {**state, "answer": "ì§ˆë¬¸ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "evidence": []}
        
        try:
            # ì‚¬ìš©ì ì„¤ë¬¸ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            survey_data = self._get_user_survey_data(user_id)
            
            # ì„¤ë¬¸ì§€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì¸í™”ëœ ì¶”ì²œ ìƒì„±
            response_content = self._generate_personalized_recommendation(question, survey_data)
            
            return {
                **state,
                "answer": response_content,
                "evidence": [],
                "agent_type": "recommendation",
                "metadata": {
                    "query": question,
                    "user_id": user_id,
                    "agent_type": "recommendation",
                    "survey_data_used": survey_data is not None
                }
            }
        except Exception as e:
            return {
                **state,
                "answer": f"ì¶”ì²œ ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "evidence": [],
                "agent_type": "recommendation"
            }
    
    def _get_user_survey_data(self, user_id: Optional[int]) -> Optional[Dict[str, Any]]:
        """ì‚¬ìš©ìì˜ ì„¤ë¬¸ì§€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        if not user_id:
            return None
        
        try:
            return self.sqlite_meta.get_user_survey_response(user_id)
        except Exception as e:
            print(f"ì„¤ë¬¸ì§€ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def _generate_personalized_recommendation(self, question: str, survey_data: Optional[Dict[str, Any]]) -> str:
        """ì„¤ë¬¸ì§€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì¸í™”ëœ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not survey_data:
            return f"ì¶”ì²œ ì—ì´ì „íŠ¸ê°€ '{question}' ìš”ì²­ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ê°œì¸í™”ëœ ì¶”ì²œì„ ìœ„í•´ ì´ˆê¸° ì„¤ë¬¸ì§€ë¥¼ ì™„ë£Œí•´ì£¼ì„¸ìš”."
        
        # ì„¤ë¬¸ì§€ ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
        job_field = survey_data.get('job_field', '')
        job_field_other = survey_data.get('job_field_other', '')
        interests = survey_data.get('interests', [])
        help_preferences = survey_data.get('help_preferences', [])
        custom_keywords = survey_data.get('custom_keywords', '')
        
        # ì§ì—… ë¶„ì•¼ì— ë”°ë¥¸ ë§ì¶¤í˜• ì¶”ì²œ
        job_recommendations = self._get_job_based_recommendations(job_field, job_field_other)
        
        # ê´€ì‹¬ì‚¬ì— ë”°ë¥¸ ì¶”ì²œ
        interest_recommendations = self._get_interest_based_recommendations(interests)
        
        # ë„ì›€ ë°›ê³  ì‹¶ì€ ì˜ì—­ì— ë”°ë¥¸ ì¶”ì²œ
        help_recommendations = self._get_help_based_recommendations(help_preferences)
        
        # ì‚¬ìš©ì ì •ì˜ í‚¤ì›Œë“œ í™œìš©
        keyword_recommendations = self._get_keyword_based_recommendations(custom_keywords)
        
        # ëª¨ë“  ì¶”ì²œì„ ì¢…í•©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        response_parts = []
        
        if job_recommendations:
            response_parts.append(f"ğŸ“‹ {job_field} ë¶„ì•¼ ê´€ë ¨: {job_recommendations}")
        
        if interest_recommendations:
            response_parts.append(f"ğŸ¯ ê´€ì‹¬ì‚¬ ê¸°ë°˜: {interest_recommendations}")
        
        if help_recommendations:
            response_parts.append(f"ğŸ’¡ ë„ì›€ ì˜ì—­: {help_recommendations}")
        
        if keyword_recommendations:
            response_parts.append(f"ğŸ” ë§ì¶¤ í‚¤ì›Œë“œ: {keyword_recommendations}")
        
        if not response_parts:
            return f"'{question}'ì— ëŒ€í•œ ê°œì¸í™”ëœ ì¶”ì²œì„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤. ì„¤ë¬¸ì§€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ì œì•ˆì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        return f"'{question}'ì— ëŒ€í•œ ê°œì¸í™”ëœ ì¶”ì²œì…ë‹ˆë‹¤:\n\n" + "\n\n".join(response_parts)
    
    def _get_job_based_recommendations(self, job_field: str, job_field_other: str) -> str:
        """ì§ì—… ë¶„ì•¼ì— ë”°ë¥¸ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤."""
        job_recommendations = {
            "student": "í•™ìŠµ ìë£Œ, ì—°êµ¬ ë…¼ë¬¸, í•™ìˆ  ìë£Œë¥¼ ì¶”ì²œë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "developer": "ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œ, ê°œë°œ ë„êµ¬, í”„ë¡œê·¸ë˜ë° ìë£Œë¥¼ ì¶”ì²œë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "designer": "ë””ìì¸ íŠ¸ë Œë“œ, ì°½ì‘ ì˜ê°, ë””ìì¸ ë„êµ¬ë¥¼ ì¶”ì²œë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "planner": "ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ, ë§ˆì¼€íŒ… ìë£Œ, ê¸°íš ë„êµ¬ë¥¼ ì¶”ì²œë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "researcher": "ì—°êµ¬ ìë£Œ, í•™ìˆ  ë…¼ë¬¸, ì‹¤í—˜ ë°ì´í„°ë¥¼ ì¶”ì²œë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "other": f"'{job_field_other}' ë¶„ì•¼ì— íŠ¹í™”ëœ ìë£Œë¥¼ ì¶”ì²œë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }
        
        return job_recommendations.get(job_field, "ì „ë¬¸ ë¶„ì•¼ì— ë§ëŠ” ìë£Œë¥¼ ì¶”ì²œë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    async def run_periodic_analysis(self, user_id: int, recommendation_type: str = 'scheduled') -> (bool, str):
        """
        ì§€ë‚œ 1ì£¼ì¼ê°„ì˜ ì‚¬ìš©ì ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤.
        :return: (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€) íŠœí”Œ
        """
        logger.info(f"ì‚¬ìš©ì {user_id}ì— ëŒ€í•œ ì£¼ê¸°ì  ë¶„ì„ ì‹œì‘ (íƒ€ì…: {recommendation_type})...")
        
        try:
            # 0. ì‚¬ìš©ì ì„¤ë¬¸ ë°ì´í„° ë¯¸ë¦¬ ì¡°íšŒ (ê´€ì‹¬ì‚¬ ê¸°ë°˜ ê°€ì¤‘ì¹˜ì— ì‚¬ìš©)
            survey_data = self._get_user_survey_data(user_id)

            # 1. ì§€ë‚œ 1ì£¼ì¼ ë°ì´í„° ì¡°íšŒ
            one_week_ago = int((datetime.now() - timedelta(days=7)).timestamp())
            files = self.sqlite_meta.get_collected_files_since(user_id, one_week_ago)
            history = self.sqlite_meta.get_collected_browser_history_since(user_id, one_week_ago)
            data_source = "ìµœê·¼ í™œë™"

            # 1ì°¨ í´ë°±: ì „ì²´ ê¸°ê°„ ë°ì´í„° ì¡°íšŒ
            if not files and not history:
                logger.info(f"User {user_id}: ì§€ë‚œ 1ì£¼ì¼ê°„ ë°ì´í„°ê°€ ì—†ì–´ ì „ì²´ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.")
                files = self.sqlite_meta.get_collected_files(user_id)
                history = self.sqlite_meta.get_collected_browser_history(user_id)
                data_source = "ì „ì²´ í™œë™"

            # 2. ì¶”ì²œì— ì‚¬ìš©í•  "ë¬¸ì„œ" ë¦¬ìŠ¤íŠ¸ êµ¬ì„± (íŒŒì¼ + ë¸Œë¼ìš°ì € + ì„¤ë¬¸)
            documents: List[str] = []
            
            # 2-1. íŒŒì¼ ì´ë¦„ / ì¹´í…Œê³ ë¦¬ / ë‚´ìš© í”„ë¦¬ë·°ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê²°í•©
            if files:
                for f in files:
                    parts = [
                        f.get('file_name', ''),
                        f.get('file_category', ''),
                        f.get('content_preview', '')
                    ]
                    doc_text = " ".join(p for p in parts if p)
                    if doc_text.strip():
                        documents.append(doc_text)

            # 2-2. ë¸Œë¼ìš°ì € ê¸°ë¡ ì œëª©ì„ ë¬¸ì„œë¡œ ì‚¬ìš©
            if history:
                for h in history:
                    title = h.get('title', '')
                    if title:
                        documents.append(title)

            # 2-3. 2ì°¨ í´ë°±: ì„¤ë¬¸ ë°ì´í„° ê¸°ë°˜ ë¬¸ì„œ êµ¬ì„± (í™œë™ ë¡œê·¸ê°€ ê±°ì˜ ì—†ì„ ë•Œ)
            if not documents:
                logger.info(f"User {user_id}: í™œë™ ë°ì´í„°ê°€ ì—†ì–´ ì„¤ë¬¸ ë°ì´í„°ë¡œ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤.")
                if survey_data:
                    data_source = "ì„¤ë¬¸"
                    job_field = survey_data.get('job_field_other', '') or survey_data.get('job_field', '')
                    interests = survey_data.get('interests', [])
                    custom_keywords_str = survey_data.get('custom_keywords', '')

                    # ì„¤ë¬¸ ê¸°ë°˜ ë¬¸ì„œë“¤ êµ¬ì„±
                    if job_field:
                        documents.append(str(job_field))
                    for it in interests or []:
                        documents.append(str(it))
                    if custom_keywords_str:
                        documents.append(custom_keywords_str)
            
            if not documents:
                msg = f"User {user_id}: ë¶„ì„í•  ë°ì´í„°ê°€ ì „í˜€ ì—†ìŠµë‹ˆë‹¤."
                logger.info(msg)
                return False, "ë¶„ì„í•  ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
            # 3. ì„¤ë¬¸ ê¸°ë°˜ ê´€ì‹¬ì‚¬ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ê³ ë ¤í•œ í‚¤ì›Œë“œ ì„ íƒ (ì—†ìœ¼ë©´ TF-IDFë¡œ í´ë°±)
            top_keywords = self._select_keywords_by_interest_similarity(documents, survey_data, top_n=5)

            if not top_keywords:
                msg = f"User {user_id}: ì£¼ìš” í™œë™ ì£¼ì œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                logger.info(msg)
                return False, "ë°ì´í„°ì—ì„œ ì£¼ìš” í™œë™ ì£¼ì œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            # 4. LLMì„ ì‚¬ìš©í•´ "ì¶”ì²œ í‚¤ì›Œë“œ" 1ê°œ ìƒì„± (í•µì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜)
            recommended_keyword = self._generate_llm_recommendation_keyword(top_keywords)

            # 5. ì¶”ì²œ ìƒì„± ë° ì €ì¥
            title = "í™œë™ ìš”ì•½ ë° ì¶”ì²œ"
            if recommended_keyword:
                content = (
                    f"'{data_source}' ë°ì´í„°ë¥¼ ë¶„ì„í•œ ê²°ê³¼, '{', '.join(top_keywords)}' ì£¼ì œì— ë§ì€ ê´€ì‹¬ì„ ë³´ì´ì…¨ìŠµë‹ˆë‹¤. "
                    f"ì´ ì¤‘ì—ì„œë„ íŠ¹íˆ '{recommended_keyword}' ì£¼ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë” ê¹Šì´ ìˆëŠ” ì •ë³´ë¥¼ ì°¾ì•„ë³´ê±°ë‚˜ "
                    f"ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•´ ë³´ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤."
                )
            else:
                content = (
                    f"'{data_source}' ë°ì´í„°ë¥¼ ë¶„ì„í•œ ê²°ê³¼, '{', '.join(top_keywords)}' ì£¼ì œì— ë§ì€ ê´€ì‹¬ì„ ë³´ì´ì…¨ìŠµë‹ˆë‹¤. "
                    f"ì´ì™€ ê´€ë ¨í•˜ì—¬ ë” ê¹Šì´ ìˆëŠ” ì •ë³´ë¥¼ ì°¾ì•„ë³´ê±°ë‚˜ ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•´ ë³´ëŠ” ê²ƒì€ ì–´ë– ì‹ ê°€ìš”?"
                )
            
            # TODO: ì¤‘ë³µ ì¶”ì²œ ë°©ì§€ ë¡œì§ ì¶”ê°€
            
            if self.sqlite_meta.insert_recommendation(user_id, title, content, recommendation_type=recommendation_type):
                logger.info(f"âœ… User {user_id}: ìƒˆë¡œìš´ ì£¼ê°„ ì¶”ì²œì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {top_keywords}")
                return True, "ìƒˆë¡œìš´ ì¶”ì²œì„ ì„±ê³µì ìœ¼ë¡œ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
            else:
                logger.error(f"âŒ User {user_id}: ì¶”ì²œ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False, "ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ì²œì„ ì €ì¥í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            logger.error(f"User {user_id} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            return False, f"ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def _extract_keywords_from_text(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ë¶ˆìš©ì–´ ì²˜ë¦¬ ê°•í™”)"""
        if not text:
            return []
        
        import re
        
        # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ì†Œë¬¸ì ë³€í™˜ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        # URL ì œê±°
        text = re.sub(r'http\S+', '', text)
        processed_text = re.sub(r'[^ \wê°€-í£]', ' ', text.lower())
        words = processed_text.split()
        
        # 2. ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ í™•ì¥ (ì“°ë ˆê¸° ë°ì´í„° í•„í„°ë§)
        stopwords = {
            # ì¼ë°˜ì ì¸ ì˜ì–´ ë¶ˆìš©ì–´
            'and', 'the', 'for', 'with', 'this', 'that', 'from', 'to', 'in', 'on', 'at', 'by', 'of', 'is', 'are', 'was', 'were',
            'it', 'its', 'as', 'be', 'have', 'has', 'had', 'do', 'does', 'did', 'can', 'could', 'will', 'would', 'should',
            'about', 'which', 'what', 'when', 'where', 'who', 'how', 'why', 'not', 'no', 'yes', 'or', 'but', 'if', 'so',
            
            # ì›¹/ë¸Œë¼ìš°ì € ê´€ë ¨ ì“°ë ˆê¸° ë°ì´í„°
            'http', 'https', 'www', 'com', 'net', 'org', 'co', 'kr', 'ac', 'io', 'html', 'htm', 'php', 'jsp', 'asp',
            'google', 'naver', 'daum', 'kakao', 'youtube', 'facebook', 'twitter', 'instagram', 'linkedin', 'github',
            'login', 'signin', 'signup', 'logout', 'signout', 'account', 'password', 'id', 'user', 'profile',
            'search', 'query', 'find', 'result', 'results', 'index', 'home', 'main', 'site', 'web', 'page',
            'new', 'tab', 'window', 'untitled', 'loading', 'error', '404', '500', 'server', 'client', 'localhost',
            'docs', 'drive', 'sheet', 'slide', 'document', 'file', 'folder', 'image', 'video', 'audio',
            'pdf', 'doc', 'docx', 'ppt', 'pptx', 'xls', 'xlsx', 'csv', 'txt', 'hwp', 'zip', 'rar', 'tar', 'gz',
            'receipt', 'success', 'final', 'draft', 'copy', 'sample',
            
            # ì¼ë°˜ì ì¸ í•œê¸€ ë¶ˆìš©ì–´ ë° ì›¹ ê´€ë ¨
            'ë°', 'ìœ„í•œ', 'í†µí•´', 'ê´€ë ¨', 'ëŒ€í•œ', 'ì…ë‹ˆë‹¤', 'ìœ¼ë¡œ', 'ì—ì„œ', 'í•˜ê³ ', 'ìˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”',
            'êµ¬ê¸€', 'ë„¤ì´ë²„', 'ë‹¤ìŒ', 'ì¹´ì¹´ì˜¤', 'ìœ íŠœë¸Œ', 'í˜ì´ìŠ¤ë¶', 'íŠ¸ìœ„í„°', 'ì¸ìŠ¤íƒ€ê·¸ë¨', 'ë§í¬ë“œì¸', 'ê¹ƒí—ˆë¸Œ',
            'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…', 'ë¡œê·¸ì•„ì›ƒ', 'ê³„ì •', 'ë¹„ë°€ë²ˆí˜¸', 'ì•„ì´ë””', 'ì‚¬ìš©ì', 'í”„ë¡œí•„', 'ë‚´ì •ë³´',
            'ê²€ìƒ‰', 'í†µí•©ê²€ìƒ‰', 'ê²°ê³¼', 'ë©”ì¸', 'í™ˆ', 'ì‚¬ì´íŠ¸', 'ì›¹í˜ì´ì§€', 'í˜ì´ì§€', 'ìƒˆíƒ­', 'ë¬´ì œ',
            'ë¡œë”©ì¤‘', 'ì˜¤ë¥˜', 'ì—ëŸ¬', 'ì„œë²„', 'í´ë¼ì´ì–¸íŠ¸', 'íŒŒì¼', 'í´ë”', 'ë¬¸ì„œ', 'ì´ë¯¸ì§€', 'ë™ì˜ìƒ',
            'ì €ì¥', 'ì—´ê¸°', 'ë‹«ê¸°', 'ìˆ˜ì •', 'ì‚­ì œ', 'ì·¨ì†Œ', 'í™•ì¸', 'ì™„ë£Œ', 'ì„¤ì •', 'ê´€ë¦¬', 'ë³´ê¸°', 'ë”ë³´ê¸°',
            'ì„±ê³µ', 'ì¸ì¦', 'ê²°ì œ', 'ì˜ìˆ˜ì¦', 'ìµœì¢…', 'ì‚¬ë³¸', 'ì„ì‹œ', 'ë°±ì—…', 'ê²€í† ', 'ì´ˆì•ˆ', 'ìƒ˜í”Œ'
        }
        
        # íŒŒì¼ í™•ì¥ì/í˜•ì‹, ì´ë©”ì¼, ë¬´ì‘ìœ„ ë¬¸ìì—´ ì œê±°ë¥¼ ìœ„í•œ íŒ¨í„´
        file_ext_pattern = re.compile(r'\.(pdf|docx?|pptx?|xlsx?|xls|csv|md|txt|jpg|jpeg|png|gif|zip|rar|hwp)$', re.IGNORECASE)
        email_pattern = re.compile(r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$')
        random_string_pattern = re.compile(r'^[a-zA-Z0-9]{6,}$')
        alpha_pattern = re.compile(r'^[a-zA-Zê°€-í£]{2,}$')

        keywords = []
        for word in words:
            # 2ê¸€ì ë¯¸ë§Œ ì œì™¸
            if len(word) < 2:
                continue
                
            # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
            if word.isdigit():
                continue
                
            # ë¶ˆìš©ì–´ ì œì™¸
            if word in stopwords:
                continue
                
            # í•œê¸€ ì¡°ì‚¬/ì–´ë¯¸ ê°„ë‹¨ ì œê±° (ëê¸€ì ê¸°ë°˜)
            # ì™„ë²½í•˜ì§„ ì•Šì§€ë§Œ 'ë°ì´í„°ë¥¼' -> 'ë°ì´í„°' ì •ë„ë¡œ ì •ì œ
            if re.match(r'[ê°€-í£]+', word):
                original_word = word
                # í”í•œ ì¡°ì‚¬ë“¤
                josa_list = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë¡œ', 'ìœ¼ë¡œ', 'ê³¼', 'ì™€', 'ë„', 'ë§Œ', 'ì„œ', 'ê»˜']
                for josa in josa_list:
                    if word.endswith(josa) and len(word) > len(josa) + 1: # ì¡°ì‚¬ ë–¼ê³ ë„ 2ê¸€ì ì´ìƒì¼ ë•Œë§Œ
                        word = word[:-len(josa)]
                        break
                
                # ë‹¤ì‹œ í•œë²ˆ ë¶ˆìš©ì–´ ì²´í¬ (ì¡°ì‚¬ ë–¼ê³  ë‚˜ë‹ˆ ë¶ˆìš©ì–´ì¼ ìˆ˜ ìˆìŒ)
                if word in stopwords:
                    continue
            
            # íŒŒì¼ í™•ì¥ì/í˜•ì‹ ì œê±°
            if file_ext_pattern.search(word):
                continue

            # ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
            if email_pattern.match(word):
                continue

            # ë¬´ì‘ìœ„ ë¬¸ìì—´(ì˜ë¬¸/ìˆ«ì í˜¼í•© 6ì ì´ìƒ) ì œê±°
            if random_string_pattern.match(word) and not alpha_pattern.match(word):
                continue

            keywords.append(word)
            
        return list(set(keywords))

    def _extract_keywords_tfidf(self, documents: List[str], top_n: int = 30) -> List[str]:
        """
        (í´ë°±ìš©) TF-IDFë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì§‘í•©ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ì‹ ê·œ ë¡œì§ì—ì„œëŠ” `_compute_tfidf_term_scores`ë¥¼ ì‚¬ìš©í•˜ê³ ,
        ì´ í•¨ìˆ˜ëŠ” ì„¤ë¬¸ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ìœ ì‚¬ë„ ê³„ì‚°ì´ ì–´ë ¤ìš´ ê²½ìš°ì—ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        term_scores, _, _ = self._compute_tfidf_term_scores(documents)
        if not term_scores:
            return []
        sorted_terms = sorted(term_scores.items(), key=lambda x: x[1], reverse=True)
        return [t for t, _ in sorted_terms[:top_n]]
    def _compute_tfidf_term_scores(self, documents: List[str]) -> (Dict[str, float], Optional["TfidfVectorizer"], Dict[str, int]):
        """
        ì „ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ëŒ€ìƒìœ¼ë¡œ TF-IDFë¥¼ ê³„ì‚°í•˜ê³ ,
        ê° ë‹¨ì–´(í† í°)ë³„ ì¤‘ìš”ë„ ì ìˆ˜ì™€ ë¬¸ì„œ ë¹ˆë„(Doc Frequency)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if not documents:
            return {}, None, {}

        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
        except ImportError:
            logger.warning("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ TF-IDF ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # ê°„ë‹¨í•œ ë¹ˆë„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´
            all_tokens: List[str] = []
            for doc in documents:
                all_tokens.extend(self._extract_keywords_from_text(doc))
            counter = Counter(all_tokens)
            scores = {k: float(v) for k, v in counter.items()}
            doc_freqs = {k: len(documents) for k in counter.keys()}
            return scores, None, doc_freqs

        try:
            # 1) ê° ë¬¸ì„œë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ í‚¤ì›Œë“œ í† í° ë¬¸ìì—´ë¡œ ë³€í™˜
            processed_docs: List[str] = []
            for doc in documents:
                tokens = self._extract_keywords_from_text(doc)
                if tokens:
                    processed_docs.append(" ".join(tokens))

            if not processed_docs:
                return {}, None, {}

            # 2) TF-IDF ë²¡í„°í™” (ë‹¨ì–´ ë‹¨ìœ„)
            vectorizer = TfidfVectorizer(
                token_pattern=r"(?u)\b\w+\b",
                max_features=2000,
                norm="l2",
            )
            tfidf_matrix = vectorizer.fit_transform(processed_docs)
            feature_names = vectorizer.get_feature_names_out()
            doc_freq_arr = (tfidf_matrix > 0).sum(axis=0).A1

            # 3) ê° ë‹¨ì–´ì˜ ì „ì²´ ë¬¸ì„œì—ì„œì˜ ì¤‘ìš”ë„ í•©ì‚°
            scores_arr = tfidf_matrix.sum(axis=0).A1
            term_scores = {term: float(score) for term, score in zip(feature_names, scores_arr)}
            doc_freqs = {term: int(df) for term, df in zip(feature_names, doc_freq_arr)}

            return term_scores, vectorizer, doc_freqs

        except Exception as e:
            logger.error(f"TF-IDF ê¸°ë°˜ ë‹¨ì–´ ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            return {}, None, {}

    def _select_keywords_by_interest_similarity(
        self,
        documents: List[str],
        survey_data: Optional[Dict[str, Any]],
        top_n: int = 5
    ) -> List[str]:
        """
        (ê°œì„ ëœ ë²„ì „)
        1) ë¬¸ì„œ ì „ì²´ì— ëŒ€í•œ TF-IDF ì ìˆ˜(term_scores)ë¥¼ ê³„ì‚°í•˜ê³ ,
        2) ì„¤ë¬¸ì—ì„œ ì œê³µëœ ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œì™€ ê° ë‹¨ì–´ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬,
        3) weighted_fit_score = 1 * tfidf_score_norm + 9 * cosine_similarity
           ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ Nê°œ í‚¤ì›Œë“œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
        ì„¤ë¬¸ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìœ¼ë©´ TF-IDF ê²°ê³¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.
        """
        # 1. TF-IDF ê¸°ë°˜ ë‹¨ì–´ ì¤‘ìš”ë„ ê³„ì‚°
        term_scores, vectorizer, doc_freqs = self._compute_tfidf_term_scores(documents)
        if not term_scores:
            return self._extract_keywords_tfidf(documents, top_n=top_n)

        total_docs = max(len(documents), 1)

        # TF-IDF ìƒìœ„ ì¼ë¶€ë§Œ í›„ë³´ë¡œ ì‚¬ìš© (ë„ˆë¬´ ë§ì€ ë‹¨ì–´ëŠ” ë…¸ì´ì¦ˆì´ë¯€ë¡œ ì œí•œ)
        sorted_terms = sorted(term_scores.items(), key=lambda x: x[1], reverse=True)
        candidate_keywords = []
        for term, _ in sorted_terms:
            df_ratio = doc_freqs.get(term, 0) / total_docs
            if df_ratio >= 0.4:  # ë¹„ì •ìƒì ìœ¼ë¡œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ ì œê±°
                continue
            candidate_keywords.append(term)
            if len(candidate_keywords) >= 200:
                break

        # 2. ì„¤ë¬¸ ê¸°ë°˜ ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        interest_terms: List[str] = []
        if survey_data:
            job_field = survey_data.get('job_field_other', '') or survey_data.get('job_field', '')
            if job_field:
                interest_terms.append(str(job_field))

            interests = survey_data.get('interests', [])
            if isinstance(interests, list):
                for it in interests:
                    if it:
                        interest_terms.append(str(it))

            custom_keywords_str = survey_data.get('custom_keywords', '')
            if custom_keywords_str:
                interest_terms.extend(self._extract_keywords_from_text(custom_keywords_str))

        # ì„¤ë¬¸ ì •ë³´ê°€ ì „í˜€ ì—†ìœ¼ë©´ TF-IDFë¡œ í´ë°±
        interest_terms = list({t for t in interest_terms if t})
        if not interest_terms:
            return self._extract_keywords_tfidf(documents, top_n=top_n)

        # vectorizerê°€ ì—†ë‹¤ë©´(=ë¹ˆë„ ê¸°ë°˜ í´ë°±) TF-IDF ìˆœìœ„ë§Œ ì‚¬ìš©
        if vectorizer is None:
            return self._extract_keywords_tfidf(documents, top_n=top_n)

        try:
            # 3. ì„¤ë¬¸ ê´€ì‹¬ì‚¬ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì³ ë²¡í„°í™”
            interest_text = " ".join(interest_terms)
            interest_vec = vectorizer.transform([interest_text])  # (1, D)
            if interest_vec.nnz == 0:
                return self._extract_keywords_tfidf(documents, top_n=top_n)

            # TF-IDF ì ìˆ˜ ì •ê·œí™” (0~1 ë²”ìœ„)
            max_tfidf = max(term_scores.values()) if term_scores else 1.0
            if max_tfidf == 0:
                max_tfidf = 1.0

            scored_candidates = []
            for term in candidate_keywords:
                base_score = term_scores.get(term, 0.0)
                tfidf_norm = base_score / max_tfidf

                term_vec = vectorizer.transform([term])
                if term_vec.nnz == 0:
                    cosine_sim = 0.0
                else:
                    # norm='l2' ì´ë¯€ë¡œ dot product == cosine similarity
                    cosine_sim = float(term_vec @ interest_vec.T)

                weighted_fit_score = 1.0 * tfidf_norm + 9.0 * cosine_sim
                scored_candidates.append((term, weighted_fit_score))

            # ì˜ë¯¸ ì—†ëŠ”(ì ìˆ˜ ë„ˆë¬´ ë‚®ì€) í›„ë³´ ì œê±°
            scored_candidates = [item for item in scored_candidates if item[1] > 0.05]
            if not scored_candidates:
                return self._extract_keywords_tfidf(documents, top_n=top_n)

            scored_candidates.sort(key=lambda x: x[1], reverse=True)
            top = [w for w, _ in scored_candidates[:top_n]]
            return top

        except Exception as e:
            logger.error(f"ê´€ì‹¬ì‚¬ ê¸°ë°˜ ê°€ì¤‘ì¹˜ í‚¤ì›Œë“œ ì„ íƒ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            return self._extract_keywords_tfidf(documents, top_n=top_n)


    def _generate_llm_recommendation_keyword(self, base_keywords: List[str]) -> Optional[str]:
        """
        ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ LLM(Gemini)ì„ í˜¸ì¶œí•˜ì—¬
        ì‚¬ìš©ìê°€ ì•ìœ¼ë¡œ ë” íƒìƒ‰í•´ ë³´ë©´ ì¢‹ì„ ë§Œí•œ 'ì¶”ì²œ í‚¤ì›Œë“œ' 1ê°œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        if not base_keywords:
            return None

        # Gemini API í‚¤ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        try:
            import google.generativeai as genai
        except ImportError:
            logger.warning("google-generativeaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ LLM ì¶”ì²œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        if not settings.GEMINI_API_KEY:
            logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ LLM ì¶”ì²œ í‚¤ì›Œë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        try:
            genai.configure(api_key=settings.GEMINI_API_KEY)
            
            # Safety Settings: ëª¨ë“  ì°¨ë‹¨ í•„í„° í•´ì œ
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            model = genai.GenerativeModel(
                model_name="gemini-2.5-flash",
                generation_config={
                    "temperature": 0.7,
                    "top_p": 0.8,
                    "top_k": 40,
                    "max_output_tokens": 32,
                    "response_mime_type": "text/plain",
                },
                safety_settings=safety_settings,
            )

            keywords_str = ", ".join(base_keywords)
            prompt = (
                "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ë¥¼ ë¶„ì„í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤.\n"
                f"ë‹¤ìŒì€ ì‚¬ìš©ìê°€ ìµœê·¼ ê´€ì‹¬ì„ ë³´ì¸ í‚¤ì›Œë“œ ëª©ë¡ì…ë‹ˆë‹¤: {keywords_str}\n\n"
                "ì´ í‚¤ì›Œë“œë“¤ì„ ì¢…í•©í–ˆì„ ë•Œ, ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ íƒìƒ‰í•´ë³´ë©´ ì¢‹ì„ ë§Œí•œ 'ì—°ê´€ ì£¼ì œ'ë¥¼ ë‹¨ í•œ ë‹¨ì–´ë¡œ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\n"
                "ê·œì¹™:\n"
                "1. ë°˜ë“œì‹œ í•œêµ­ì–´ ë‹¨ì–´ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
                "2. ë¶€ê°€ ì„¤ëª…, ê¸°í˜¸, ê³µë°± ì—†ì´ ì˜¤ì§ ë‹¨ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
                "3. ì„ ì •ì ì´ê±°ë‚˜ ìœ„í—˜í•œ ë‹¨ì–´ëŠ” ì œì™¸í•˜ê³ , í•™ìˆ ì /ì‹¤ìš©ì  ì£¼ì œë¥¼ ìš°ì„ í•˜ì„¸ìš”."
            )

            response = model.generate_content(prompt, request_options={"timeout": 10})

            # Gemini ì‘ë‹µ ì•ˆì „ íŒŒì‹±
            candidates = getattr(response, "candidates", None) or []
            if not candidates:
                # Safety Feedback ë“± í™•ì¸
                feedback = getattr(response, "prompt_feedback", None)
                logger.warning("LLM ì¶”ì²œ í‚¤ì›Œë“œ ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. prompt_feedback=%s", feedback)
                return None

            candidate = candidates[0]
            finish_reason = getattr(candidate, "finish_reason", None)
            # 0: FINISH, 1: MAX_TOKENS, 2: SAFETY, 3: RECITATION, 4: OTHER
            # SAFETY(2) ë“±ìœ¼ë¡œ ë§‰í˜€ë„ í˜¹ì‹œë‚˜ í…ìŠ¤íŠ¸ê°€ ì¼ë¶€ë¼ë„ ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ë„ë¡ ì‹œë„í•  ìˆ˜ ìˆìœ¼ë‚˜,
            # ë³´í†µ ë§‰íˆë©´ partsê°€ ì•„ì˜ˆ ì—†ìŒ. ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ë¦¬í„´.
            if finish_reason not in (None, 0, 1):  # MAX_TOKENSê¹Œì§€ëŠ” í—ˆìš©
                logger.warning("LLM ì‘ë‹µì´ finish_reason=%s ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (Safety Filter ë“±)", finish_reason)
                # ê°•ì œ ë°˜í™˜í•˜ì§€ ì•Šê³  ì•„ë˜ íŒŒì‹± ë¡œì§ ì‹œë„ (í˜¹ì‹œ ëª¨ë¥¼ í…ìŠ¤íŠ¸ ì¡´ì¬ ê°€ëŠ¥ì„±)
            
            content_parts = getattr(getattr(candidate, "content", None), "parts", None) or []
            extracted_chunks: List[str] = []
            for part in content_parts:
                text_chunk = getattr(part, "text", None)
                if text_chunk:
                    extracted_chunks.append(text_chunk)

            if not extracted_chunks:
                # fallback: response.text accessor (ì˜ˆì™¸ ë°©ì§€)
                try:
                    fallback_text = (response.text or "").strip()
                except Exception:
                    fallback_text = ""
                if fallback_text:
                    extracted_chunks.append(fallback_text)

            if not extracted_chunks:
                return None

            text = "\n".join(extracted_chunks).strip()
            if not text:
                return None

            # ì²« ì¤„ë§Œ ì‚¬ìš©í•˜ê³ , ì–‘ìª½ ê³µë°± ë° ë”°ì˜´í‘œ ì œê±°
            keyword = text.splitlines()[0].strip().strip("\"'â€œâ€'â€˜â€™")
            # ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì´ìƒí•œ ê²½ìš°ëŠ” ë¬´ì‹œ
            if not keyword or len(keyword) > 20:
                return None
            return keyword

        except Exception as e:
            logger.error(f"LLM ì¶”ì²œ í‚¤ì›Œë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            return None

    def _get_interest_based_recommendations(self, interests: List[str]) -> str:
        """ê´€ì‹¬ì‚¬ì— ë”°ë¥¸ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not interests:
            return ""
        
        interest_mapping = {
            "tech": "ìµœì‹  IT ê¸°ìˆ , í”„ë¡œê·¸ë˜ë°, ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ",
            "finance": "ê²½ì œ ë™í–¥, íˆ¬ì ì •ë³´, ê¸ˆìœµ ë‰´ìŠ¤",
            "ai": "ì¸ê³µì§€ëŠ¥ ì—°êµ¬, ë¨¸ì‹ ëŸ¬ë‹, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤",
            "design": "ë””ìì¸ íŠ¸ë Œë“œ, ì°½ì‘ ì˜ê°, ì˜ˆìˆ  ì‘í’ˆ",
            "marketing": "ë§ˆì¼€íŒ… ì „ëµ, ë¸Œëœë”©, ê´‘ê³  ìº í˜ì¸",
            "productivity": "ìƒì‚°ì„± ë„êµ¬, ì‹œê°„ ê´€ë¦¬, ìê¸°ê³„ë°œ",
            "health": "ê±´ê°• ì •ë³´, ìš´ë™ ë£¨í‹´, ì›°ë¹™ íŒ",
            "travel": "ì—¬í–‰ ì •ë³´, ë¬¸í™” ì²´í—˜, ê´€ê´‘ì§€"
        }
        
        recommendations = [interest_mapping.get(interest, interest) for interest in interests]
        return f"ê´€ì‹¬ ì£¼ì œ '{', '.join(recommendations)}'ì— ê´€ë ¨ëœ ìë£Œë¥¼ ì¶”ì²œë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    
    def _get_help_based_recommendations(self, help_preferences: List[str]) -> str:
        """ë„ì›€ ë°›ê³  ì‹¶ì€ ì˜ì—­ì— ë”°ë¥¸ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not help_preferences:
            return ""
        
        help_mapping = {
            "work_search": "ì—…ë¬´ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ë° ìš”ì•½ ë„êµ¬",
            "inspiration": "ì°½ì˜ì  ì•„ì´ë””ì–´ì™€ ì˜ê°ì„ ì£¼ëŠ” ìë£Œ",
            "writing": "ê¸€ì“°ê¸° ë³´ì¡° ë„êµ¬ì™€ í…œí”Œë¦¿",
            "learning": "ê°œì¸ í•™ìŠµì„ ìœ„í•œ êµìœ¡ ìë£Œì™€ ê°•ì˜"
        }
        
        recommendations = [help_mapping.get(pref, pref) for pref in help_preferences]
        return f"'{', '.join(recommendations)}' ì˜ì—­ì—ì„œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    
    def _get_keyword_based_recommendations(self, custom_keywords: str) -> str:
        """ì‚¬ìš©ì ì •ì˜ í‚¤ì›Œë“œì— ë”°ë¥¸ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not custom_keywords:
            return ""
        
        return f"'{custom_keywords}'ì™€ ê´€ë ¨ëœ ë§ì¶¤í˜• ìë£Œë¥¼ ì¶”ì²œë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    
    async def process_async(self, user_input: str, user_id: Optional[int] = None) -> AgentResponse:
        """ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ)"""
        try:
            # ê°„ë‹¨í•œ ì¶”ì²œ ê´€ë ¨ ì‘ë‹µ
            response_content = f"ì¶”ì²œ ì—ì´ì „íŠ¸ê°€ '{user_input}' ìš”ì²­ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ëŠ” ê¸°ë³¸ ì‘ë‹µë§Œ ì œê³µí•©ë‹ˆë‹¤."
            
            return AgentResponse(
                success=True,
                content=response_content,
                agent_type=self.agent_type,
                metadata={
                    "query": user_input,
                    "user_id": user_id,
                    "agent_type": "recommendation"
                }
            )
        except Exception as e:
            return AgentResponse(
                success=False,
                content=f"ì¶”ì²œ ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                agent_type=self.agent_type
            )
    
    def _analyze_recommendation_type(self, user_input: str) -> str:
        """ì¶”ì²œ íƒ€ì…ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        input_lower = user_input.lower()
        
        if any(word in input_lower for word in ["ì§€ì‹", "knowledge", "ì •ë³´"]):
            return "knowledge"
        elif any(word in input_lower for word in ["ì½˜í…ì¸ ", "content", "ìë£Œ"]):
            return "content"
        elif any(word in input_lower for word in ["í•™ìŠµ", "learning", "ê²½ë¡œ", "path"]):
            return "learning_path"
        else:
            return "knowledge"
    
    async def _recommend_knowledge(self, user_id: int, user_input: str) -> AgentResponse:
        """ì§€ì‹ ê¸°ë°˜ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # SQLiteì—ì„œ ì‚¬ìš©ì ë°ì´í„° ì¡°íšŒ
            collected_files = self.sqlite_meta.get_collected_files(user_id)
            collected_browser = self.sqlite_meta.get_collected_browser_history(user_id)
            collected_apps = self.sqlite_meta.get_collected_apps(user_id)
            
            # ì‚¬ìš©ì ê´€ì‹¬ì‚¬ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
            interests = self._extract_interests_from_data(collected_files, collected_browser, collected_apps)
            
            # ê¸°ë³¸ ì¶”ì²œ ë¡œì§
            recommendations = self._generate_basic_recommendations(interests, user_input)
            
            return AgentResponse(
                success=True,
                content=f"ì¶”ì²œ ê²°ê³¼: {recommendations}",
                agent_type=self.agent_type,
                metadata={"user_id": user_id, "interests": interests}
            )
            
        except Exception as e:
            return AgentResponse(
                success=False,
                content=f"ì§€ì‹ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                agent_type=self.agent_type
            )
    
    async def _recommend_content(self, user_id: int, user_input: str) -> AgentResponse:
        """ì½˜í…ì¸  ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ì‚¬ìš©ì ê´€ì‹¬ì‚¬ ê¸°ë°˜ìœ¼ë¡œ ì›¹ ê²€ìƒ‰
            interests = await self._get_user_interests(user_id)
            
            if not interests:
                return AgentResponse(
                    success=True,
                    content="ì‚¬ìš©ì ê´€ì‹¬ì‚¬ë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ì–´ ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    agent_type=self.agent_type
                )
            
            # ê´€ì‹¬ì‚¬ë³„ë¡œ ì›¹ ê²€ìƒ‰
            recommendations = []
            for interest in interests[:3]:  # ìƒìœ„ 3ê°œ ê´€ì‹¬ì‚¬ë§Œ ì‚¬ìš©
                search_result = await self.execute_tool(
                    "web_search_tool",
                    query=f"{interest} ê´€ë ¨ ìµœì‹  ì •ë³´",
                    max_results=2
                )
                
                if search_result.success:
                    for item in search_result.data:
                        recommendations.append({
                            "title": item.get("title", ""),
                            "url": item.get("url", ""),
                            "snippet": item.get("snippet", ""),
                            "interest": interest,
                            "type": "web_content"
                        })
            
            return AgentResponse(
                success=True,
                content={
                    "recommendations": recommendations[:10],  # ìµœëŒ€ 10ê°œ
                    "user_interests": interests
                },
                agent_type=self.agent_type,
                tools_used=["web_search_tool"],
                metadata={"recommendation_type": "content"}
            )
            
        except Exception as e:
            return AgentResponse(
                success=False,
                content=f"ì½˜í…ì¸  ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}",
                agent_type=self.agent_type
            )
    
    async def _recommend_learning_path(self, user_id: int, user_input: str) -> AgentResponse:
        """í•™ìŠµ ê²½ë¡œ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ì‚¬ìš©ì í˜„ì¬ ìˆ˜ì¤€ê³¼ ëª©í‘œ ë¶„ì„
            user_profile = await self._analyze_user_profile(user_id)
            
            # í•™ìŠµ ê²½ë¡œ ìƒì„±
            learning_path = self._generate_learning_path(user_profile, user_input)
            
            return AgentResponse(
                success=True,
                content={
                    "learning_path": learning_path,
                    "user_profile": user_profile
                },
                agent_type=self.agent_type,
                metadata={"recommendation_type": "learning_path"}
            )
            
        except Exception as e:
            return AgentResponse(
                success=False,
                content=f"í•™ìŠµ ê²½ë¡œ ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}",
                agent_type=self.agent_type
            )
    
    def _extract_interests_from_data(self, collected_files: List[Dict[str, Any]], 
                                   collected_browser: List[Dict[str, Any]], 
                                   collected_apps: List[Dict[str, Any]]) -> List[str]:
        """ìˆ˜ì§‘ëœ ë°ì´í„°ì—ì„œ ì‚¬ìš©ì ê´€ì‹¬ì‚¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        interests = []
        
        # íŒŒì¼ëª…ì—ì„œ ê´€ì‹¬ì‚¬ ì¶”ì¶œ
        for file_info in collected_files:
            file_name = file_info.get('file_name', '').lower()
            file_path = file_info.get('file_path', '').lower()
            
            # ì¼ë°˜ì ì¸ ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œ
            interest_keywords = [
                "python", "javascript", "java", "c++", "machine learning", "ai", "data science",
                "web development", "mobile development", "database", "cloud", "devops",
                "í”„ë¡œê·¸ë˜ë°", "ì½”ë”©", "ê°œë°œ", "í•™ìŠµ", "í”„ë¡œì íŠ¸", "ì•Œê³ ë¦¬ì¦˜", "ìë£Œêµ¬ì¡°"
            ]
            
            for keyword in interest_keywords:
                if keyword in file_name or keyword in file_path:
                    interests.append(keyword)
        
        # ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ì—ì„œ ê´€ì‹¬ì‚¬ ì¶”ì¶œ
        for browser_info in collected_browser:
            url = browser_info.get('url', '').lower()
            title = browser_info.get('title', '').lower()
            
            for keyword in interest_keywords:
                if keyword in url or keyword in title:
                    interests.append(keyword)
        
        # ì•± ì‚¬ìš©ì—ì„œ ê´€ì‹¬ì‚¬ ì¶”ì¶œ
        for app_info in collected_apps:
            app_name = app_info.get('app_name', '').lower()
            window_title = app_info.get('window_title', '').lower()
            
            for keyword in interest_keywords:
                if keyword in app_name or keyword in window_title:
                    interests.append(keyword)
        
        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ìˆœ ì •ë ¬
        interest_counts = {}
        for interest in interests:
            interest_counts[interest] = interest_counts.get(interest, 0) + 1
        
        sorted_interests = sorted(interest_counts.items(), key=lambda x: x[1], reverse=True)
        return [interest for interest, count in sorted_interests[:10]]  # ìƒìœ„ 10ê°œ
    
    async def _get_user_interests(self, user_id: int) -> List[str]:
        """ì‚¬ìš©ì ê´€ì‹¬ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            # SQLiteì—ì„œ ì‚¬ìš©ì ë°ì´í„° ì¡°íšŒ
            collected_files = self.sqlite_meta.get_collected_files(user_id)
            collected_browser = self.sqlite_meta.get_collected_browser_history(user_id)
            collected_apps = self.sqlite_meta.get_collected_apps(user_id)
            
            return self._extract_interests_from_data(collected_files, collected_browser, collected_apps)
        except Exception as e:
            print(f"ì‚¬ìš©ì ê´€ì‹¬ì‚¬ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _calculate_relevance_score(self, item: Dict[str, Any], interests: List[str], user_input: str) -> float:
        """ì§€ì‹ í•­ëª©ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        score = 0.0
        
        # ì‚¬ìš©ì ê´€ì‹¬ì‚¬ì™€ì˜ ë§¤ì¹­
        item_content_lower = item.get('content', '').lower()
        item_title_lower = item.get('title', '').lower()
        
        for interest in interests:
            if interest.lower() in item_content_lower:
                score += 2.0
            if interest.lower() in item_title_lower:
                score += 3.0
        
        # ì‚¬ìš©ì ì…ë ¥ê³¼ì˜ ë§¤ì¹­
        user_input_lower = user_input.lower()
        if user_input_lower in item_content_lower:
            score += 1.5
        if user_input_lower in item_title_lower:
            score += 2.0
        
        # íƒœê·¸ ë§¤ì¹­
        tags = item.get('tags', [])
        for tag in tags:
            if tag.lower() in user_input_lower:
                score += 1.0
        
        return score
    
    def _generate_basic_recommendations(self, interests: List[str], user_input: str) -> List[Dict[str, Any]]:
        """ê¸°ë³¸ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤."""
        recommendations = []
        
        # ê´€ì‹¬ì‚¬ ê¸°ë°˜ ì¶”ì²œ
        for interest in interests[:5]:  # ìƒìœ„ 5ê°œ ê´€ì‹¬ì‚¬
            recommendations.append({
                "type": "interest_based",
                "title": f"{interest} ê´€ë ¨ í•™ìŠµ ìë£Œ",
                "description": f"{interest}ì— ëŒ€í•œ í•™ìŠµ ìë£Œë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.",
                "interest": interest,
                "priority": "high"
            })
        
        # ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜ ì¶”ì²œ
        if user_input:
            recommendations.append({
                "type": "query_based",
                "title": f"'{user_input}' ê´€ë ¨ ì¶”ì²œ",
                "description": f"ì‚¬ìš©ì ì§ˆë¬¸ '{user_input}'ì— ëŒ€í•œ ê´€ë ¨ ìë£Œë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.",
                "query": user_input,
                "priority": "high"
            })
        
        return recommendations
    
    async def _analyze_user_profile(self, user_id: int) -> Dict[str, Any]:
        """ì‚¬ìš©ì í”„ë¡œí•„ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            # SQLiteì—ì„œ ì‚¬ìš©ì ë°ì´í„° ì¡°íšŒ
            collected_files = self.sqlite_meta.get_collected_files(user_id)
            collected_browser = self.sqlite_meta.get_collected_browser_history(user_id)
            collected_apps = self.sqlite_meta.get_collected_apps(user_id)
            
            # ê´€ì‹¬ì‚¬ ì¶”ì¶œ
            interests = self._extract_interests_from_data(collected_files, collected_browser, collected_apps)
            
            # ê°„ë‹¨í•œ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±
            total_interactions = len(collected_files) + len(collected_browser) + len(collected_apps)
            experience_level = self._estimate_experience_level_simple(total_interactions)
            
            return {
                "user_id": user_id,
                "username": f"User_{user_id}",
                "total_interactions": total_interactions,
                "agent_usage": {"general": total_interactions},
                "interests": interests,
                "experience_level": experience_level
            }
        except Exception as e:
            return {
                "user_id": user_id,
                "username": "Unknown",
                "total_interactions": 0,
                "agent_usage": {},
                "interests": [],
                "experience_level": "beginner"
            }
    
    def _estimate_experience_level_simple(self, total_interactions: int) -> str:
        """ì‚¬ìš©ìì˜ ê²½í—˜ ìˆ˜ì¤€ì„ ì¶”ì •í•©ë‹ˆë‹¤."""
        if total_interactions < 10:
            return "beginner"
        elif total_interactions < 50:
            return "intermediate"
        else:
            return "advanced"
    
    def _generate_learning_path(self, user_profile: Dict[str, Any], user_input: str) -> List[Dict[str, Any]]:
        """í•™ìŠµ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        experience_level = user_profile.get("experience_level", "beginner")
        interests = user_profile.get("interests", [])
        
        # ê¸°ë³¸ í•™ìŠµ ê²½ë¡œ í…œí”Œë¦¿
        learning_paths = {
            "beginner": [
                {
                    "step": 1,
                    "title": "ê¸°ì´ˆ ê°œë… í•™ìŠµ",
                    "description": "í”„ë¡œê·¸ë˜ë°ì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.",
                    "estimated_time": "2-3ì£¼",
                    "resources": ["ì˜¨ë¼ì¸ íŠœí† ë¦¬ì–¼", "ê¸°ì´ˆ êµì¬"]
                },
                {
                    "step": 2,
                    "title": "ì‹¤ìŠµ í”„ë¡œì íŠ¸",
                    "description": "ê°„ë‹¨í•œ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ì‹¤ìŠµí•©ë‹ˆë‹¤.",
                    "estimated_time": "1-2ì£¼",
                    "resources": ["ë¯¸ë‹ˆ í”„ë¡œì íŠ¸", "ì½”ë”© ì—°ìŠµ"]
                }
            ],
            "intermediate": [
                {
                    "step": 1,
                    "title": "ì‹¬í™” ê°œë… í•™ìŠµ",
                    "description": "ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ê°œë…ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
                    "estimated_time": "3-4ì£¼",
                    "resources": ["ê³ ê¸‰ êµì¬", "ì˜¨ë¼ì¸ ê°•ì˜"]
                },
                {
                    "step": 2,
                    "title": "ì‹¤ë¬´ í”„ë¡œì íŠ¸",
                    "description": "ì‹¤ë¬´ ìˆ˜ì¤€ì˜ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.",
                    "estimated_time": "4-6ì£¼",
                    "resources": ["ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸", "íŒ€ í”„ë¡œì íŠ¸"]
                }
            ],
            "advanced": [
                {
                    "step": 1,
                    "title": "ì „ë¬¸ ë¶„ì•¼ ì‹¬í™”",
                    "description": "íŠ¹ì • ë¶„ì•¼ì˜ ì „ë¬¸ ì§€ì‹ì„ ìŠµë“í•©ë‹ˆë‹¤.",
                    "estimated_time": "6-8ì£¼",
                    "resources": ["ì „ë¬¸ ì„œì ", "ì»¨í¼ëŸ°ìŠ¤ ì°¸ì„"]
                },
                {
                    "step": 2,
                    "title": "ë¦¬ë”ì‹­ ë° ë©˜í† ë§",
                    "description": "ë‹¤ë¥¸ ê°œë°œìë¥¼ ê°€ë¥´ì¹˜ê³  ë¦¬ë“œí•©ë‹ˆë‹¤.",
                    "estimated_time": "ì§€ì†ì ",
                    "resources": ["ë©˜í† ë§ í”„ë¡œê·¸ë¨", "ê¸°ìˆ  ë¸”ë¡œê·¸ ìš´ì˜"]
                }
            ]
        }
        
        base_path = learning_paths.get(experience_level, learning_paths["beginner"])
        
        # ê´€ì‹¬ì‚¬ì— ë§ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§•
        customized_path = []
        for step in base_path:
            customized_step = step.copy()
            if interests:
                customized_step["description"] += f" (ê´€ì‹¬ ë¶„ì•¼: {', '.join(interests[:3])})"
            customized_path.append(customized_step)
        
        return customized_path 