"""
RecommendationAgent - ëŠ¥ë™í˜• ì¶”ì²œ ì—ì´ì „íŠ¸ (Active Agent)

LLM(Gemini)ì´ ì‚¬ìš©ìì˜ ë¡œê·¸ì™€ ê´€ì‹¬ì‚¬(Survey)ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ íƒ€ì´ë°ì—
"ë§í’ì„  ë©”ì‹œì§€"ë¥¼ ì œì•ˆí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
"""

import json
import logging
from typing import List, Dict, Any, Optional, Tuple

import google.generativeai as genai

from ..base_agent import BaseAgent, AgentResponse
from database.sqlite import SQLite
from config.settings import settings

logger = logging.getLogger(__name__)


class RecommendationAgent(BaseAgent):
    """ëŠ¥ë™í˜• ì¶”ì²œ ì—ì´ì „íŠ¸ - LLM ê¸°ë°˜ ë¶„ì„ ë° ë§í’ì„  ë©”ì‹œì§€ ìƒì„±"""
    
    def __init__(self):
        super().__init__(
            agent_type="recommendation",
            description="ì‚¬ìš©ì í™œë™ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤."
        )
        self.sqlite = SQLite()
        self._init_llm()
    
    def _init_llm(self):
        """Gemini LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.llm_available = False
        if not settings.GEMINI_API_KEY:
            logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            genai.configure(api_key=settings.GEMINI_API_KEY)
            self.safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            self.llm_model = genai.GenerativeModel(
                model_name="gemini-2.5-flash",
                generation_config={
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40,
                    "max_output_tokens": 1024,
                    "response_mime_type": "application/json",
                },
                safety_settings=self.safety_settings,
            )
            self.llm_available = True
            logger.info("Gemini LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"Gemini LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    
    # ============================================================
    # BaseAgent Interface Implementation
    # ============================================================
    
    def process(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒíƒœë¥¼ ë°›ì•„ì„œ ì²˜ë¦¬í•˜ê³  ìˆ˜ì •ëœ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        question = state.get("question", "")
        user_id = state.get("user_id")
        
        if not question:
            return {**state, "answer": "ì§ˆë¬¸ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "evidence": []}
        
        try:
            # ì‚¬ìš©ìì—ê²Œ ëŒ€ê¸° ì¤‘ì¸ ì¶”ì²œì´ ìˆëŠ”ì§€ í™•ì¸
            pending = self.get_pending_recommendations(user_id) if user_id else []
            
            if pending:
                response_content = (
                    f"í˜„ì¬ {len(pending)}ê°œì˜ ì¶”ì²œì´ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤.\n"
                    f"ì²« ë²ˆì§¸ ì¶”ì²œ: {pending[0].get('bubble_message', 'ìƒˆë¡œìš´ ì¶”ì²œì´ ìˆì–´ìš”!')}"
                )
            else:
                response_content = "í˜„ì¬ ëŒ€ê¸° ì¤‘ì¸ ì¶”ì²œì´ ì—†ìŠµë‹ˆë‹¤. í™œë™ì„ ê³„ì†í•˜ì‹œë©´ ë§ì¶¤í˜• ì¶”ì²œì„ ì¤€ë¹„í•´ ë“œë¦´ê²Œìš”!"
            
            return {
                **state,
                "answer": response_content,
                "evidence": [],
                "agent_type": "recommendation",
                "metadata": {
                    "query": question,
                    "user_id": user_id,
                    "pending_count": len(pending)
                }
            }
        except Exception as e:
            return {
                **state,
                "answer": f"ì¶”ì²œ ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "evidence": [],
                "agent_type": "recommendation"
            }
    
    async def process_async(self, user_input: str, user_id: Optional[int] = None) -> AgentResponse:
        """ì‚¬ìš©ì ì…ë ¥ì„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            pending = self.get_pending_recommendations(user_id) if user_id else []
            
            if pending:
                content = f"ëŒ€ê¸° ì¤‘ì¸ ì¶”ì²œì´ {len(pending)}ê°œ ìˆìŠµë‹ˆë‹¤."
            else:
                content = "í˜„ì¬ ëŒ€ê¸° ì¤‘ì¸ ì¶”ì²œì´ ì—†ìŠµë‹ˆë‹¤."
            
            return AgentResponse(
                success=True,
                content=content,
                agent_type=self.agent_type,
                metadata={"user_id": user_id, "pending_count": len(pending)}
            )
        except Exception as e:
            return AgentResponse(
                success=False,
                content=f"ì¶”ì²œ ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                agent_type=self.agent_type
            )
    
    # ============================================================
    # Core Active Analysis Methods
    # ============================================================
    
    async def run_active_analysis(self, user_id: int) -> Tuple[bool, str]:
        """
        ëŠ¥ë™í˜• ë¶„ì„ ì‹¤í–‰ - ì£¼ê¸°ì ìœ¼ë¡œ í˜¸ì¶œë˜ì–´ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Returns:
            Tuple[bool, str]: (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
        """
        logger.info(f"ì‚¬ìš©ì {user_id}ì— ëŒ€í•œ ëŠ¥ë™í˜• ë¶„ì„ ì‹œì‘...")
        
        if not self.llm_available:
            return False, "LLM ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        try:
            # Step 1: Data Preparation
            browser_logs = self.sqlite.get_unprocessed_browser_logs(user_id)
            app_logs = self.sqlite.get_unprocessed_app_logs(user_id)
            
            if not browser_logs and not app_logs:
                logger.info(f"User {user_id}: ë¶„ì„í•  ìƒˆë¡œìš´ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False, "ë¶„ì„í•  ìƒˆë¡œìš´ í™œë™ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            # ì°¸ì¡° ë°ì´í„° ì¡°íšŒ
            blacklist = self.sqlite.get_blacklist(user_id)
            user_interests = self.sqlite.get_user_interests(user_id)
            survey_data = self.sqlite.get_survey_response(user_id)
            
            # Step 2: LLM Analysis & Decision
            analysis_result = await self._analyze_with_llm(
                browser_logs=browser_logs,
                app_logs=app_logs,
                blacklist=blacklist,
                user_interests=user_interests,
                survey_data=survey_data
            )
            
            # Step 3: Process Results
            browser_log_ids = [log['id'] for log in browser_logs]
            app_log_ids = [log['id'] for log in app_logs]
            
            # ë¡œê·¸ë¥¼ ì²˜ë¦¬ë¨ìœ¼ë¡œ í‘œì‹œ (ì¶”ì²œ ìƒì„± ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´)
            if browser_log_ids:
                self.sqlite.mark_browser_logs_processed(browser_log_ids)
            if app_log_ids:
                self.sqlite.mark_app_logs_processed(app_log_ids)
            
            if not analysis_result or not analysis_result.get('should_recommend'):
                logger.info(f"User {user_id}: LLMì´ ì¶”ì²œí•  ë§Œí•œ ë‚´ìš©ì´ ì—†ë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤.")
                return False, "í˜„ì¬ ì¶”ì²œí•  ë§Œí•œ íŠ¹ë³„í•œ í™œë™ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            # ì¶”ì²œ ìƒì„±
            rec_id = self.sqlite.create_recommendation(
                user_id=user_id,
                trigger_type=analysis_result.get('trigger_type', 'new_interest'),
                keyword=analysis_result.get('keyword', ''),
                bubble_message=analysis_result.get('bubble_message', ''),
                related_keywords=analysis_result.get('related_keywords', [])
            )
            
            if rec_id <= 0:
                logger.error(f"User {user_id}: ì¶”ì²œ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False, "ì¶”ì²œ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
            # ìƒˆë¡œìš´ ê´€ì‹¬ì‚¬ë¼ë©´ ë“±ë¡
            if analysis_result.get('trigger_type') == 'new_interest':
                keyword = analysis_result.get('keyword')
                if keyword:
                    self.sqlite.upsert_interest(
                        user_id=user_id,
                        keyword=keyword,
                        score=0.6,
                        source='active_analysis'
                    )
            
            logger.info(f"âœ… User {user_id}: ìƒˆë¡œìš´ ì¶”ì²œ ìƒì„± ì™„ë£Œ (ID: {rec_id})")
            return True, f"ìƒˆë¡œìš´ ì¶”ì²œì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {analysis_result.get('keyword')}"
            
        except Exception as e:
            logger.error(f"User {user_id} ëŠ¥ë™í˜• ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            return False, f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    async def _analyze_with_llm(
        self,
        browser_logs: List[Dict[str, Any]],
        app_logs: List[Dict[str, Any]],
        blacklist: List[str],
        user_interests: List[Dict[str, Any]],
        survey_data: Optional[Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        # ë¡œê·¸ ìš”ì•½ ìƒì„±
        log_summary = self._prepare_log_summary(browser_logs, app_logs)
        
        # ê¸°ì¡´ ê´€ì‹¬ì‚¬ ëª©ë¡
        existing_interests = [item['keyword'] for item in user_interests]
        
        # Survey ì •ë³´ ì¶”ì¶œ
        survey_info = ""
        if survey_data:
            job_field = survey_data.get('job_field_other') or survey_data.get('job_field', '')
            interests = survey_data.get('interests', [])
            custom_keywords = survey_data.get('custom_keywords', '')
            survey_info = f"""
ì„¤ë¬¸ì§€ ì •ë³´:
- ì§ì—…/ë¶„ì•¼: {job_field}
- ê´€ì‹¬ ë¶„ì•¼: {', '.join(interests) if interests else 'ì—†ìŒ'}
- ì»¤ìŠ¤í…€ í‚¤ì›Œë“œ: {custom_keywords if custom_keywords else 'ì—†ìŒ'}
"""
        
        prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ í™œë™ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• ì¶”ì²œì„ ì œì•ˆí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ì‚¬ìš©ì í™œë™ ë¡œê·¸
{log_summary}

## ê¸°ì¡´ ê´€ì‹¬ì‚¬
{', '.join(existing_interests) if existing_interests else 'ë“±ë¡ëœ ê´€ì‹¬ì‚¬ ì—†ìŒ'}

{survey_info}

## ë¸”ë™ë¦¬ìŠ¤íŠ¸ (ì¶”ì²œ ì œì™¸ í‚¤ì›Œë“œ)
{', '.join(blacklist) if blacklist else 'ì—†ìŒ'}

## ë¶„ì„ ì§€ì‹œì‚¬í•­
1. ë¡œê·¸ì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œì™€ ì£¼ì œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
2. ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í‚¤ì›Œë“œëŠ” ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.
3. ë‹¤ìŒ ë‘ ê°€ì§€ ì¼€ì´ìŠ¤ ì¤‘ í•˜ë‚˜ë¥¼ íŒë‹¨í•˜ì„¸ìš”:
   - **Case A (new_interest)**: ê¸°ì¡´ ê´€ì‹¬ì‚¬ì— ì—†ë˜ ìƒˆë¡œìš´ ì£¼ì œê°€ ë°œê²¬ëœ ê²½ìš°
   - **Case B (periodic_expansion)**: ê¸°ì¡´ ê´€ì‹¬ì‚¬ë¥¼ ë” ê¹Šê²Œ íƒêµ¬í•˜ëŠ” í™œë™ì´ ê°ì§€ëœ ê²½ìš°
4. ì¶”ì²œí•  ë§Œí•œ ë‚´ìš©ì´ ì—†ë‹¤ë©´ should_recommendë¥¼ falseë¡œ ì„¤ì •í•˜ì„¸ìš”.
5. ì¶”ì²œ ì‹œ, ì‚¬ìš©ìì—ê²Œ ê±´ë„¬ **ì¹œê·¼í•œ í•œêµ­ì–´ ë§í’ì„  ë©”ì‹œì§€**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
   - ì˜ˆì‹œ: "ìš”ì¦˜ Pythonì— ê´€ì‹¬ì´ ë§ìœ¼ì‹œë„¤ìš”! ê´€ë ¨ ìë£Œë¥¼ ì°¾ì•„ë³¼ê¹Œìš”? ğŸ"

## ì¶œë ¥ í˜•ì‹ (JSON)
{{
    "should_recommend": true/false,
    "trigger_type": "new_interest" ë˜ëŠ” "periodic_expansion",
    "keyword": "í•µì‹¬ í‚¤ì›Œë“œ (í•œ ë‹¨ì–´ ë˜ëŠ” ì§§ì€ êµ¬ë¬¸)",
    "related_keywords": ["ê´€ë ¨", "í‚¤ì›Œë“œ", "ëª©ë¡"],
    "bubble_message": "ì¹œê·¼í•œ í•œêµ­ì–´ ë§í’ì„  ë©”ì‹œì§€",
    "reasoning": "íŒë‹¨ ê·¼ê±° (ë‚´ë¶€ìš©)"
}}

ë§Œì•½ ì¶”ì²œí•  ë‚´ìš©ì´ ì—†ë‹¤ë©´:
{{
    "should_recommend": false,
    "reasoning": "ì¶”ì²œí•˜ì§€ ì•ŠëŠ” ì´ìœ "
}}
"""

        try:
            response = self.llm_model.generate_content(
                prompt,
                request_options={"timeout": 30}
            )
            
            # ì‘ë‹µ íŒŒì‹±
            result_text = self._extract_llm_response_text(response)
            if not result_text:
                return None
            
            # JSON íŒŒì‹±
            result = json.loads(result_text)
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"LLM ì‘ë‹µ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            logger.error(f"LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            return None
    
    def _prepare_log_summary(
        self,
        browser_logs: List[Dict[str, Any]],
        app_logs: List[Dict[str, Any]]
    ) -> str:
        """ë¡œê·¸ ë°ì´í„°ë¥¼ LLM í”„ë¡¬í”„íŠ¸ìš© ìš”ì•½ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        lines = []
        
        if browser_logs:
            lines.append("### ë¸Œë¼ìš°ì € ë°©ë¬¸ ê¸°ë¡")
            for log in browser_logs[:30]:  # ìµœëŒ€ 30ê°œ
                title = log.get('title', 'ì œëª© ì—†ìŒ')
                url = log.get('url', '')
                # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
                domain = url.split('/')[2] if url.startswith('http') and len(url.split('/')) > 2 else url
                lines.append(f"- {title} ({domain})")
        
        if app_logs:
            lines.append("\n### ì•± ì‚¬ìš© ê¸°ë¡")
            for log in app_logs[:20]:  # ìµœëŒ€ 20ê°œ
                app_name = log.get('app_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
                window_title = log.get('window_title', '')
                duration = log.get('duration_seconds', 0)
                if window_title:
                    lines.append(f"- {app_name}: {window_title} ({duration}ì´ˆ)")
                else:
                    lines.append(f"- {app_name} ({duration}ì´ˆ)")
        
        return '\n'.join(lines) if lines else "í™œë™ ë¡œê·¸ ì—†ìŒ"
    
    def _extract_llm_response_text(self, response) -> Optional[str]:
        """Gemini ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            candidates = getattr(response, "candidates", None) or []
            if not candidates:
                return None
            
            candidate = candidates[0]
            content_parts = getattr(getattr(candidate, "content", None), "parts", None) or []
            
            extracted_chunks = []
            for part in content_parts:
                text_chunk = getattr(part, "text", None)
                if text_chunk:
                    extracted_chunks.append(text_chunk)
            
            if not extracted_chunks:
                # Fallback: response.text
                try:
                    return (response.text or "").strip()
                except Exception:
                    return None
            
            return "\n".join(extracted_chunks).strip()
            
        except Exception as e:
            logger.error(f"LLM ì‘ë‹µ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    # ============================================================
    # Interaction Handling
    # ============================================================
    
    async def handle_response(self, recommendation_id: int, action: str) -> Tuple[bool, str]:
        """
        UIì—ì„œ ì‚¬ìš©ìê°€ ì¶”ì²œì— ì‘ë‹µí–ˆì„ ë•Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            recommendation_id: ì¶”ì²œ ID
            action: 'accept' ë˜ëŠ” 'reject'
        
        Returns:
            Tuple[bool, str]: (ì„±ê³µ ì—¬ë¶€, ê²°ê³¼ ë©”ì‹œì§€ ë˜ëŠ” ë¦¬í¬íŠ¸)
        """
        logger.info(f"ì¶”ì²œ {recommendation_id}ì— ëŒ€í•œ ì‘ë‹µ ì²˜ë¦¬: {action}")
        
        # ì¶”ì²œ ì •ë³´ ì¡°íšŒ
        recommendation = self.sqlite.get_recommendation(recommendation_id)
        if not recommendation:
            return False, "ì¶”ì²œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        if action == 'accept':
            return await self._handle_accept(recommendation)
        elif action == 'reject':
            return await self._handle_reject(recommendation)
        else:
            return False, f"ì•Œ ìˆ˜ ì—†ëŠ” ì•¡ì…˜: {action}"
    
    async def _handle_accept(self, recommendation: Dict[str, Any]) -> Tuple[bool, str]:
        """ì¶”ì²œ ìˆ˜ë½ ì²˜ë¦¬ - ë¦¬í¬íŠ¸ ìƒì„±"""
        rec_id = recommendation['id']
        user_id = recommendation['user_id']
        keyword = recommendation.get('keyword', '')
        related_keywords = recommendation.get('related_keywords', [])
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self.sqlite.update_recommendation_status(rec_id, 'accepted')
        
        # LLMìœ¼ë¡œ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        if self.llm_available:
            report_content = await self._generate_report(
                keyword=keyword,
                related_keywords=related_keywords,
                user_id=user_id
            )
        else:
            report_content = f"## {keyword} ê´€ë ¨ ì •ë³´\n\nê´€ì‹¬ í‚¤ì›Œë“œ: {keyword}\nê´€ë ¨ í‚¤ì›Œë“œ: {', '.join(related_keywords)}\n\n*LLM ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ìƒì„¸ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.*"
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        self.sqlite.update_recommendation_report(rec_id, report_content)
        
        # ê´€ì‹¬ì‚¬ ì ìˆ˜ ìƒí–¥ ì¡°ì •
        if keyword:
            self.sqlite.upsert_interest(
                user_id=user_id,
                keyword=keyword,
                score=0.8,
                source='user_accepted'
            )
        
        logger.info(f"âœ… ì¶”ì²œ {rec_id} ìˆ˜ë½ ì²˜ë¦¬ ì™„ë£Œ")
        return True, report_content
    
    async def _handle_reject(self, recommendation: Dict[str, Any]) -> Tuple[bool, str]:
        """ì¶”ì²œ ê±°ì ˆ ì²˜ë¦¬ - ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€"""
        rec_id = recommendation['id']
        user_id = recommendation['user_id']
        keyword = recommendation.get('keyword', '')
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self.sqlite.update_recommendation_status(rec_id, 'rejected')
        
        # í‚¤ì›Œë“œ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        if keyword:
            self.sqlite.add_to_blacklist(user_id, keyword)
            logger.info(f"í‚¤ì›Œë“œ '{keyword}'ê°€ ì‚¬ìš©ì {user_id}ì˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        logger.info(f"âŒ ì¶”ì²œ {rec_id} ê±°ì ˆ ì²˜ë¦¬ ì™„ë£Œ")
        return True, "ì¶”ì²œì´ ê±°ì ˆë˜ì—ˆìŠµë‹ˆë‹¤. í•´ë‹¹ í‚¤ì›Œë“œëŠ” ë” ì´ìƒ ì¶”ì²œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    
    async def _generate_report(
        self,
        keyword: str,
        related_keywords: List[str],
        user_id: int
    ) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # ì‚¬ìš©ì ê´€ì‹¬ì‚¬ì™€ ì„¤ë¬¸ ì •ë³´ ì¡°íšŒ
        survey_data = self.sqlite.get_survey_response(user_id)
        
        context = ""
        if survey_data:
            job_field = survey_data.get('job_field_other') or survey_data.get('job_field', '')
            if job_field:
                context = f"ì‚¬ìš©ì ì§ì—…/ë¶„ì•¼: {job_field}"
        
        prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ ë§ì¶¤í˜• ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## í‚¤ì›Œë“œ ì •ë³´
- í•µì‹¬ í‚¤ì›Œë“œ: {keyword}
- ê´€ë ¨ í‚¤ì›Œë“œ: {', '.join(related_keywords) if related_keywords else 'ì—†ìŒ'}
{f'- {context}' if context else ''}

## ìš”ì²­
ìœ„ í‚¤ì›Œë“œì— ëŒ€í•´ ì‚¬ìš©ìê°€ ì•Œë©´ ì¢‹ì„ **í•µì‹¬ ì •ë³´ë¥¼ 3~5ì¤„ë¡œ ìš”ì•½**í•´ì„œ Markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.

## ì‘ì„± ê°€ì´ë“œë¼ì¸
1. ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±
2. í•µì‹¬ ê°œë…ì´ë‚˜ ìµœì‹  íŠ¸ë Œë“œ ìœ„ì£¼ë¡œ ì„¤ëª…
3. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ í™œìš©í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
4. í•„ìš”í•˜ë‹¤ë©´ ê°„ë‹¨í•œ íŒì´ë‚˜ ì¶”ì²œ ë¦¬ì†ŒìŠ¤ í¬í•¨

## ì¶œë ¥ í˜•ì‹
Markdown í˜•ì‹ì˜ ìš”ì•½ ë¦¬í¬íŠ¸ (3~5ì¤„)
"""

        try:
            # ë¦¬í¬íŠ¸ ìƒì„±ìš© ëª¨ë¸ ì„¤ì • (ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶œë ¥)
            report_model = genai.GenerativeModel(
                model_name="gemini-2.5-flash",
                generation_config={
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_output_tokens": 512,
                    "response_mime_type": "text/plain",
                },
                safety_settings=self.safety_settings,
            )
            
            response = report_model.generate_content(
                prompt,
                request_options={"timeout": 20}
            )
            
            report_text = self._extract_llm_response_text(response)
            if report_text:
                return report_text
            
        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        
        # Fallback ë¦¬í¬íŠ¸
        return f"""## {keyword} ğŸ“Œ

**{keyword}**ì— ëŒ€í•´ ê´€ì‹¬ì„ ê°€ì§€ê³  ê³„ì‹œë„¤ìš”!

ê´€ë ¨ í‚¤ì›Œë“œ: {', '.join(related_keywords) if related_keywords else 'ì—†ìŒ'}

ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ì±„íŒ…ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”! ğŸ”
"""
    
    # ============================================================
    # UI Support Methods
    # ============================================================
    
    def get_pending_recommendations(self, user_id: int) -> List[Dict[str, Any]]:
        """
        ëŒ€ê¸° ì¤‘ì¸ ì¶”ì²œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
        
        Returns:
            status='pending'ì¸ ì¶”ì²œ ëª©ë¡
        """
        try:
            return self.sqlite.get_pending_recommendations(user_id)
        except Exception as e:
            logger.error(f"ëŒ€ê¸° ì¤‘ ì¶”ì²œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def get_recommendation(self, recommendation_id: int) -> Optional[Dict[str, Any]]:
        """ì¶”ì²œ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            return self.sqlite.get_recommendation(recommendation_id)
        except Exception as e:
            logger.error(f"ì¶”ì²œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def get_all_recommendations(self, user_id: int, limit: int = 50) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ìì˜ ëª¨ë“  ì¶”ì²œ ë‚´ì—­ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            return self.sqlite.get_all_recommendations(user_id, limit)
        except Exception as e:
            logger.error(f"ëª¨ë“  ì¶”ì²œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    # ============================================================
    # Legacy Compatibility (run_periodic_analysis wrapper)
    # ============================================================
    
    async def run_periodic_analysis(self, user_id: int, recommendation_type: str = 'scheduled') -> Tuple[bool, str]:
        """
        ê¸°ì¡´ run_periodic_analysis ë©”ì„œë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼.
        ë‚´ë¶€ì ìœ¼ë¡œ run_active_analysisë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        """
        return await self.run_active_analysis(user_id)
