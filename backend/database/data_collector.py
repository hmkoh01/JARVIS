#!/usr/bin/env python3
import os
import sys
from pathlib import Path
import aiohttp
from bs4 import BeautifulSoup
import shutil
import time
import json
import sqlite3
import psutil
import threading
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import asyncio
import hashlib
from PIL import ImageGrab
import platform

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬(backend)ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
backend_dir = Path(__file__).parent.parent.absolute()
if str(backend_dir) not in sys.path:
    sys.path.insert(0, str(backend_dir))

from config.settings import settings
from .repository import Repository
from .sqlite_meta import SQLiteMeta
from .document_parser import DocumentParser
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from agents.chatbot_agent.rag.models.bge_m3_embedder import BGEM3Embedder

logger = logging.getLogger(__name__)

def init_worker_logging():
    """
    ProcessPoolExecutor ì›Œì»¤ì˜ ë¡œê¹…ì„ ì™„ì „íˆ ì–µì œí•˜ì—¬
    ë¶ˆí•„ìš”í•œ INFO ë¡œê·¸(ëª¨ë“ˆ ì´ˆê¸°í™” ë“±) ìŠ¤íŒ¸ì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì˜ ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.WARNING)
    
    # ê¸°ì¡´ì˜ ëª¨ë“  í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë¡œê·¸ ë°©ì§€)
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # WARNING ë ˆë²¨ì˜ ê°„ë‹¨í•œ ì½˜ì†” í•¸ë“¤ëŸ¬ë§Œ ì¶”ê°€
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)
    console_handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s'))
    root_logger.addHandler(console_handler)
    
    # ìì‹ ë¡œê±°ë“¤ë„ WARNING ë ˆë²¨ë¡œ ì„¤ì •
    for logger_name in ['backend.config.logging_config', '__main__']:
        logging.getLogger(logger_name).setLevel(logging.WARNING)

# -----------------------------------------------------------------------------
# FileCollector
# -----------------------------------------------------------------------------
class FileCollector:
    """ì‚¬ìš©ì ë“œë¼ì´ë¸Œì˜ íŒŒì¼ë“¤ì„ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.logger = logger.getChild(f"FileCollector[user={user_id}]")
        self.sqlite_meta = SQLiteMeta()
        self.supported_extensions = {
            'document': ['.txt', '.doc', '.docx', '.pdf', '.md', '.rtf', '.odt', '.tex'],
            'spreadsheet': ['.xls', '.xlsx', '.csv', '.ods', '.tsv'],
            'presentation': ['.ppt', '.pptx', '.odp', '.key'],
            'code': ['.py', '.js', '.ts', '.jsx', '.tsx', '.html', '.css', '.scss', '.java', '.cpp', '.c', '.h', 
                     '.cs', '.php', '.rb', '.go', '.rs', '.swift', '.kt', '.r', '.m', '.sh', '.bat', '.ps1',
                     '.sql', '.json', '.xml', '.yaml', '.yml', '.toml', '.ini', '.cfg', '.conf'],
            'note': ['.note', '.notes', '.evernote', '.onenote'],
            'ebook': ['.epub', '.mobi', '.azw', '.azw3'],
        }
        self.allowed_extensions = {ext for exts in self.supported_extensions.values() for ext in exts}

    def _get_directory_size(self, path: str) -> int:
        """ì¬ê·€ì ìœ¼ë¡œ ë””ë ‰í† ë¦¬ì˜ ì „ì²´ í¬ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        total_size = 0
        try:
            for dirpath, dirnames, filenames in os.walk(path):
                for f in filenames:
                    fp = os.path.join(dirpath, f)
                    if not os.path.islink(fp):
                        try:
                            total_size += os.path.getsize(fp)
                        except (OSError, FileNotFoundError):
                            continue
        except PermissionError:
            return 0
        return total_size

    def _format_size(self, size_bytes: int) -> str:
        """ë°”ì´íŠ¸ë¥¼ ì½ê¸° ì¢‹ì€ í˜•íƒœ(KB, MB, GB)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if size_bytes == 0:
            return "(0 bytes)"
        power = 1024
        n = 0
        power_labels = {0: 'bytes', 1: 'KB', 2: 'MB', 3: 'GB', 4: 'TB'}
        while size_bytes >= power and n < len(power_labels) - 1:
            size_bytes /= power
            n += 1
        return f"({size_bytes:.1f} {power_labels[n]})"

    def get_file_category(self, file_path: str) -> str:
        ext = Path(file_path).suffix.lower()
        for category, extensions in self.supported_extensions.items():
            if ext in extensions: return category
        return 'other'

    def should_skip_directory(self, dir_path: str) -> bool:
        skip_patterns = ['Windows', 'Program Files', '$Recycle.Bin', '.git', 'node_modules', '__pycache__', 'AppData']
        return any(part in Path(dir_path).parts for part in skip_patterns)

    def calculate_file_hash(self, file_path: str) -> str:
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                hash_md5.update(f.read(1024 * 1024))
            return hash_md5.hexdigest()
        except: return f"error_{int(time.time())}"

    def is_file_modified(self, file_path: str, last_modified: datetime) -> bool:
        stored_modified = self.sqlite_meta.get_file_last_modified(file_path)
        return stored_modified is None or last_modified > stored_modified

    def get_user_folders(self, calculate_size: bool = True) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ì í™ˆ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  í´ë”ë¥¼ ìŠ¤ìº”í•˜ê³  í¬ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
        folders = []
        base_path = os.path.join(os.path.expanduser("~"), "Desktop")
        
        try:
            with os.scandir(base_path) as it:
                for entry in it:
                    if entry.is_dir() and not entry.is_symlink() and not entry.name.startswith('.'):
                        if self.should_skip_directory(entry.path):
                            continue
                        try:
                            stat = entry.stat()
                            
                            if calculate_size:
                                dir_size = self._get_directory_size(entry.path)
                                size_formatted = self._format_size(dir_size) if dir_size is not None else "(í¬ê¸° ê³„ì‚° ì‹¤íŒ¨)"
                            else:
                                dir_size = None
                                size_formatted = "(í¬ê¸° ë¯¸ê³„ì‚°)"

                            folders.append({
                                'name': entry.name,
                                'path': entry.path,
                                'size_formatted': size_formatted,
                                'modified_date': datetime.fromtimestamp(stat.st_mtime)
                            })
                        except (OSError, PermissionError):
                            continue
        except Exception as e:
            self.logger.error("ì‚¬ìš©ì í´ë”ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", e, exc_info=True)

        return sorted(folders, key=lambda x: x['name'].lower())

    def collect_files_from_drive(
        self,
        incremental: bool,
        manager: 'DataCollectionManager',
        selected_folders: Optional[List[str]],
        progress_bounds: Tuple[float, float] = (0.0, 50.0)
    ) -> List[Dict[str, Any]]:
        paths_to_scan = []
        if selected_folders is None:
            # "ì „ì²´ ì‚¬ìš©ì í´ë” ìŠ¤ìº”"ì´ ì„ íƒëœ ê²½ìš°, ê¸°ë³¸ í´ë” ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            default_folders = self.get_user_folders()
            paths_to_scan = [folder['path'] for folder in default_folders]
        else:
            paths_to_scan = selected_folders
        
        if not paths_to_scan: 
            self.logger.warning("âš ï¸ ìŠ¤ìº”í•  í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        collected_files = []
        total_scanned = 0
        skipped_by_extension = 0
        skipped_by_hash = 0
        
        progress_start, progress_end = progress_bounds
        progress_range = max(progress_end - progress_start, 0.0)

        if manager:
            manager.progress = progress_start

        total_paths = len(paths_to_scan)

        for i, folder_path in enumerate(paths_to_scan):
            # ê²½ë¡œë¥¼ ìš´ì˜ì²´ì œì— ë§ê²Œ ì •ê·œí™”í•˜ì—¬ ê²½ë¡œ êµ¬ë¶„ì ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
            normalized_path = os.path.normpath(folder_path)
            
            if manager and total_paths > 0: 
                # progress_start ~ progress_end ë²”ìœ„ì—ì„œ ì§„í–‰ë¥  ê³„ì‚°
                manager.progress = progress_start + ((i + 1) / total_paths) * progress_range
                # ì •ê·œí™”ëœ ê²½ë¡œë¥¼ ì‚¬ìš©í•´ í´ë” ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
                manager.progress_message = f"ğŸ“ ìŠ¤ìº” ì¤‘: {Path(normalized_path).name}"
            
            try:
                # os.walkì— ì •ê·œí™”ëœ ê²½ë¡œë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
                for root, dirs, files in os.walk(normalized_path):
                    dirs[:] = [d for d in dirs if not self.should_skip_directory(os.path.join(root, d))]
                    for file in files:
                        try:
                            total_scanned += 1
                            file_path = os.path.join(root, file)
                            file_ext = Path(file_path).suffix.lower()
                            
                            if file.startswith("~$"):
                                continue
                            
                            if file_ext not in self.allowed_extensions:
                                skipped_by_extension += 1
                                continue
                            
                            stat = os.stat(file_path)
                            modified_date = datetime.fromtimestamp(stat.st_mtime)
                            if incremental and not self.is_file_modified(file_path, modified_date): continue
                            file_hash = self.calculate_file_hash(file_path)
                            
                            if self.sqlite_meta.is_file_hash_exists(file_hash):
                                skipped_by_hash += 1
                                continue
                            
                            collected_files.append({
                                'user_id': self.user_id,
                                'file_path': file_path,
                                'file_name': file,
                                'file_size': stat.st_size,
                                'file_type': file_ext,  # íŒŒì¼ í™•ì¥ì
                                'file_category': self.get_file_category(file_path),  # íŒŒì¼ ì¹´í…Œê³ ë¦¬
                                'file_hash': file_hash,
                                'modified_date': modified_date,
                                'created_date': datetime.fromtimestamp(stat.st_ctime),
                                'accessed_date': datetime.fromtimestamp(stat.st_atime)
                            })
                        except (PermissionError, OSError, FileNotFoundError): continue
            except Exception as e: 
                self.logger.error("í´ë” ìŠ¤ìº” ì˜¤ë¥˜ %s: %s", normalized_path, e, exc_info=True)
        
        # íŒŒì¼ ìˆ˜ì§‘ ì™„ë£Œ ì‹œ progress_endë¡œ ì„¤ì •
        if manager:
            if total_paths > 0:
                manager.progress = progress_end
            else:
                manager.progress = progress_start
        
        # ìˆ˜ì§‘ ê²°ê³¼ ë¡œê¹…
        self.logger.info("ğŸ“Š íŒŒì¼ ìˆ˜ì§‘ ê²°ê³¼ - ì´ ìŠ¤ìº”: %d, í™•ì¥ì ì œì™¸: %d, ì¤‘ë³µ ì œì™¸: %d, ì‹ ê·œ íŒŒì¼: %d",
                         total_scanned, skipped_by_extension, skipped_by_hash, len(collected_files))
        
        if len(collected_files) == 0 and total_scanned > 0:
            self.logger.warning("âš ï¸ ì§€ì›ë˜ëŠ” í™•ì¥ì ëª©ë¡: %s", ', '.join(sorted(self.allowed_extensions)))
        
        return collected_files

    def save_files_to_db(
        self,
        files: List[Dict[str, Any]],
        repo: Repository,
        embedder: 'BGEM3Embedder',
        parser: DocumentParser,
        manager: Optional['DataCollectionManager'] = None
    ) -> int:
        if not files:
            self.logger.warning("âš ï¸ ì €ì¥í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        if not repo:
            self.logger.error("âš ï¸ Repositoryê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 0
            
        saved_count, text_files = 0, []
        try:
            self.sqlite_meta.conn.execute("BEGIN TRANSACTION")
            for file_info in files:
                if self.sqlite_meta.insert_collected_file(file_info):
                    saved_count += 1
                    if file_info['file_category'] in ['document', 'spreadsheet', 'presentation', 'code', 'note']:
                        text_files.append(file_info)
            self.sqlite_meta.conn.commit()
            self.logger.info("âœ… SQLite íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥: %dê°œ, í…ìŠ¤íŠ¸ ì¸ë±ì‹± ëŒ€ìƒ: %dê°œ",
                             saved_count, len(text_files))
        except Exception as e: 
            self.sqlite_meta.conn.rollback()
            self.logger.error("âŒ SQLite íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: %s", e, exc_info=True)
            return 0
        
        if text_files:
            self._batch_index_text_files(text_files, repo, embedder, parser, manager)
        else:
            self.logger.warning("âš ï¸ í…ìŠ¤íŠ¸ ì¸ë±ì‹± ëŒ€ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return saved_count

    def _extract_and_save_entities(self, text: str, source_id: str, source_type: str):
        """í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥"""
        try:
            import re
            # 1. ì´ë©”ì¼ ì¶”ì¶œ
            emails = re.findall(r'[\w\.-]+@[\w\.-]+\.\w+', text)
            for email in emails:
                entity_id = self.sqlite_meta.upsert_entity(email, 'Person')
                if entity_id > 0:
                    self.sqlite_meta.add_entity_relation(entity_id, source_id, source_type, 'mentioned_in')

            # 2. ì£¼ìš” ê¸°ìˆ  í‚¤ì›Œë“œ (ì˜ˆì‹œ)
            tech_keywords = ['Python', 'Java', 'JavaScript', 'TypeScript', 'React', 'Vue', 'FastAPI', 
                           'Django', 'Spring', 'Docker', 'Kubernetes', 'AWS', 'Azure', 'GCP', 'SQL', 'NoSQL']
            for kw in tech_keywords:
                if kw.lower() in text.lower():
                    entity_id = self.sqlite_meta.upsert_entity(kw, 'Technology')
                    if entity_id > 0:
                        self.sqlite_meta.add_entity_relation(entity_id, source_id, source_type, 'mentioned_in')
        except Exception as e:
            self.logger.warning(f"ì—”í‹°í‹° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

    @staticmethod
    def _parse_single_file(file_info: Dict[str, Any], parser_ref: Any, user_id: int):
        """(í—¬í¼ í•¨ìˆ˜) ë‹¨ì¼ íŒŒì¼ì„ íŒŒì‹±. ProcessPoolExecutorì—ì„œ ì‹¤í–‰ë¨."""
        try:
            parser = None
            if parser_ref is not None:
                if isinstance(parser_ref, type):
                    parser = parser_ref()
                else:
                    parser = parser_ref
            if parser is None or not hasattr(parser, "parse_and_chunk"):
                from .document_parser import DocumentParser as _DocumentParser
                parser = _DocumentParser()

            try:
                chunk_infos = parser.parse_and_chunk(file_info['file_path'])
            except RuntimeError as e:
                return None, file_info['file_name'], f"Docling RuntimeError: {e}"
            except Exception as e:
                return None, file_info['file_name'], f"Parsing Exception: {e}"
            if not chunk_infos:
                return None, file_info['file_name'], "ì²­í¬ ì—†ìŒ"

            doc_id = f"file_{hashlib.md5(file_info['file_path'].encode()).hexdigest()}"
            file_hash = file_info.get('file_hash', '')

            texts = []
            metas = []
            for chunk in chunk_infos:
                texts.append(chunk['text'])
                metas.append({
                    'user_id': user_id,
                    'source': 'file',
                    'path': file_info['file_path'],
                    'doc_id': doc_id,
                    'chunk_id': chunk['chunk_id'],
                    'snippet': chunk['snippet'],
                    'content': chunk['text']
                })

            return (texts, metas, file_hash, file_info['file_path'], len(chunk_infos)), file_info['file_name'], None
        # [ìˆ˜ì •] ìì‹ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ëª¨ë“  ì˜¤ë¥˜ë¥¼ ì¡ê¸° ìœ„í•´ BaseException ì‚¬ìš©
        except BaseException as e:
            return None, file_info['file_name'], f"Worker setup error: {e}"

    def _process_and_upload_batch(
        self,
        repo: Repository,
        embedder: 'BGEM3Embedder',
        texts: List[str],
        metas: List[Dict[str, Any]],
        batch_size: int
    ):
        """ì²­í¬ ë°°ì¹˜ë¥¼ ë°›ì•„ ì„ë² ë”©í•˜ê³  Qdrantì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
        if not texts:
            return

        self.logger.info(
            "ğŸ§  ì²­í¬ %dê°œ ë°°ì¹˜ ì„ë² ë”© ë° ì—…ë¡œë“œ ì¤‘... (Embedding Batch Size: %d)",
            len(texts),
            batch_size
        )
        try:
            embeddings = embedder.encode_documents(texts, batch_size=batch_size)
            dense_vectors = embeddings['dense_vecs'].tolist()
            sparse_vectors = [
                embedder.convert_sparse_to_qdrant_format(lw)
                for lw in embeddings['lexical_weights']
            ]

            if repo.qdrant.upsert_vectors(metas, dense_vectors, sparse_vectors):
                self.logger.info("   ... âœ… Qdrant ì—…ë¡œë“œ ì„±ê³µ: %dê°œ", len(texts))
            else:
                self.logger.error("   ... âŒ Qdrant ì—…ë¡œë“œ ì‹¤íŒ¨")
        except Exception as e:
            # MemoryError ë“± ì¹˜ëª…ì  ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒì„¸ ë¡œê·¸
            self.logger.error("   ... âŒ ì„ë² ë”©/ì—…ë¡œë“œ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: %s", e, exc_info=True)

    def _batch_index_text_files(
        self,
        text_files: List[Dict[str, Any]],
        repo: Repository,
        embedder: 'BGEM3Embedder',
        parser: DocumentParser,
        manager: Optional['DataCollectionManager'] = None
    ):
        # ì¤‘ë³µ íŒŒì¼ ê²½ë¡œ ì œê±° (ë™ì¼ íŒŒì¼ì€ í•œ ë²ˆë§Œ íŒŒì‹±)
        seen_paths = set()
        unique_text_files: List[Dict[str, Any]] = []
        duplicate_count = 0

        for file_info in text_files:
            file_path = file_info.get('file_path')
            if not file_path:
                self.logger.debug("í…ìŠ¤íŠ¸ ì¸ë±ì‹± ëŒ€ìƒì—ì„œ file_pathê°€ ì—†ëŠ” í•­ëª©ì„ ê±´ë„ˆëœë‹ˆë‹¤: %s", file_info)
                continue

            normalized_path = os.path.normcase(os.path.abspath(file_path))
            if normalized_path in seen_paths:
                duplicate_count += 1
                continue

            seen_paths.add(normalized_path)
            unique_text_files.append(file_info)

        if duplicate_count:
            self.logger.debug("í…ìŠ¤íŠ¸ ì¸ë±ì‹± ëŒ€ìƒì—ì„œ ì¤‘ë³µ íŒŒì¼ %dê°œë¥¼ ì œì™¸í–ˆìŠµë‹ˆë‹¤.", duplicate_count)

        text_files = unique_text_files

        cpu_count = multiprocessing.cpu_count()
        self.logger.info(
            "ğŸ“ í…ìŠ¤íŠ¸ íŒŒì¼ ì¸ë±ì‹± ì‹œì‘ - íŒŒì¼ %dê°œ, ì‚¬ìš© ì½”ì–´ %dê°œ",
            len(text_files),
            cpu_count
        )
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (íŒŒì‹± ì‹œì‘)
        if manager:
            manager.progress_message = f"ğŸ“„ íŒŒì¼ íŒŒì‹± ì¤‘... (ì´ {len(text_files)}ê°œ)"

        # 1. í™˜ê²½ì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬ ì „ëµ ì„¤ì •
        is_gpu_available = getattr(embedder, "device", "cpu") == "cuda"
        embedding_batch_size = 128 if is_gpu_available else 32
        cpu_micro_batch_threshold = 5000  # CPU ëª¨ë“œì—ì„œ RAM ë³´í˜¸ìš© ì„ê³„ê°’

        all_texts: List[str] = []
        all_metas: List[Dict[str, Any]] = []
        total_chunk_count = 0
        parsed_count = 0
        failed_count = 0
        file_hash_map: Dict[str, str] = {}

        # --- 1. íŒŒì‹± (Parsing) ---
        # GPU ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´, íŒŒì‹±ì€ í•­ìƒ ProcessPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        max_workers = min(cpu_count, 8) if cpu_count > 0 else 1
        parser_ref = parser.__class__ if parser is not None else DocumentParser

        self.logger.info("--- [1/2] íŒŒì¼ íŒŒì‹± ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬) ---")
        with ProcessPoolExecutor(max_workers=max_workers, initializer=init_worker_logging) as executor:
            futures = {
                executor.submit(self._parse_single_file, file_info, parser_ref, self.user_id): file_info
                for file_info in text_files
            }

            total_files = len(text_files)
            completed_files = 0
            
            for future in as_completed(futures):
                result, file_name, error = future.result()
                completed_files += 1
                
                if result:
                    texts, metas, file_hash, file_path, chunk_count = result

                    # ëª¨ë“  ê²°ê³¼ë¥¼ RAMì˜ ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘
                    all_texts.extend(texts)
                    all_metas.extend(metas)

                    total_chunk_count += len(texts)
                    if file_hash:
                        file_hash_map[file_path] = file_hash # file_pathë¥¼ í‚¤ë¡œ ì‚¬ìš©
                    parsed_count += 1
                    self.logger.info("   âœ“ %s: %dê°œ ì²­í¬ (íŒŒì‹± ì™„ë£Œ)", file_name, chunk_count)
                    
                    # [New] ì²« ë²ˆì§¸ ì²­í¬ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ (ì„±ëŠ¥ì„ ìœ„í•´ ì¼ë¶€ë§Œ)
                    if texts:
                        self._extract_and_save_entities(texts[0][:2000], file_path, 'file')

                else:
                    failed_count += 1
                    self.logger.warning("   âœ— íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜ %s: %s", file_name, error)
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ë§¤ íŒŒì¼ë§ˆë‹¤)
                if manager and total_files > 0:
                    manager.progress_message = f"ğŸ“„ íŒŒì¼ íŒŒì‹± ì¤‘... ({completed_files}/{total_files})"

        self.logger.info(
            "ğŸ“Š íŒŒì‹± ê²°ê³¼ - ì„±ê³µ %dê°œ, ì‹¤íŒ¨ %dê°œ, ì´ ì²­í¬ %dê°œ",
            parsed_count,
            failed_count,
            total_chunk_count
        )

        if not all_texts:
            self.logger.warning("âš ï¸ ì¸ë±ì‹±í•  í…ìŠ¤íŠ¸ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # --- 2. ì„ë² ë”© & ì—…ë¡œë“œ (Embedding & Upload) ---
        # ì´ ë‹¨ê³„ì—ì„œë§Œ GPU/CPU ë¡œì§ì„ ë¶„ê¸°í•©ë‹ˆë‹¤.

        self.logger.info("--- [2/2] ì„ë² ë”© ë° ì—…ë¡œë“œ ì‹œì‘ (ëª¨ë“œ: %s) ---", "GPU" if is_gpu_available else "CPU")
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì„ë² ë”© ì‹œì‘)
        if manager:
            manager.progress_message = f"ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘... (ì´ {len(all_texts)}ê°œ ì²­í¬)"

        if is_gpu_available:
            # GPU ëª¨ë“œ: ìˆ˜ì§‘ëœ ëª¨ë“  ì²­í¬ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬ (ë¹ ë¦„)
            if all_texts:
                self.logger.info("--- GPU ëª¨ë“œ: ì´ %dê°œ ì²­í¬ ì¼ê´„ ì²˜ë¦¬ ---", len(all_texts))
                self._process_and_upload_batch(
                    repo,
                    embedder,
                    all_texts,
                    all_metas,
                    embedding_batch_size
                )
        else:
            # CPU ëª¨ë“œ: OOM ë°©ì§€ë¥¼ ìœ„í•´ ë§ˆì´í¬ë¡œ ë°°ì¹˜ë¡œ ì˜ë¼ì„œ ì²˜ë¦¬ (ì•ˆì •ì )
            self.logger.warning("--- CPU ëª¨ë“œ: %dê°œ ì²­í¬ë¥¼ %dê°œ ë‹¨ìœ„ë¡œ ë¶„í•  ì²˜ë¦¬ ---",
                               len(all_texts), cpu_micro_batch_threshold)

            total_batches = (len(all_texts) + cpu_micro_batch_threshold - 1) // cpu_micro_batch_threshold
            
            for i in range(0, len(all_texts), cpu_micro_batch_threshold):
                batch_texts = all_texts[i:i + cpu_micro_batch_threshold]
                batch_metas = all_metas[i:i + cpu_micro_batch_threshold]

                if batch_texts:
                    batch_num = i // cpu_micro_batch_threshold + 1
                    self.logger.info(f"--- CPU ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ---")
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    if manager:
                        manager.progress_message = f"ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘... (ë°°ì¹˜ {batch_num}/{total_batches})"
                    
                    self._process_and_upload_batch(
                        repo,
                        embedder,
                        batch_texts,
                        batch_metas,
                        embedding_batch_size
                    )

        # --- 3. SQLite ìƒíƒœ ì—…ë°ì´íŠ¸ ---
        if file_hash_map:
            # file_hash_mapì˜ ê°’(í•´ì‹œ)ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
            indexed_hashes = list(file_hash_map.values())
            if self.sqlite_meta.mark_files_indexed(indexed_hashes):
                self.logger.info("âœ… %dê°œ íŒŒì¼ ì¸ë±ì‹± ì™„ë£Œ í‘œì‹œ", len(indexed_hashes))
            else:
                self.logger.warning("âš ï¸ íŒŒì¼ ì¸ë±ì‹± í‘œì‹œ ì‹¤íŒ¨ (ê²€ìƒ‰ì€ ì •ìƒ ì‘ë™)")

# -----------------------------------------------------------------------------
# BrowserHistoryCollector
# -----------------------------------------------------------------------------
class BrowserHistoryCollector:
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.logger = logger.getChild(f"BrowserHistoryCollector[user={user_id}]")
        self.sqlite_meta = SQLiteMeta()
        self.browser_paths = self._get_browser_paths()
        self.browser_paths = self._get_browser_paths()
        self.parser = DocumentParser()

    def _extract_and_save_entities(self, text: str, source_id: str, source_type: str):
        """í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥ (FileCollectorì™€ ë¡œì§ ê³µìœ  ê°€ëŠ¥)"""
        try:
            import re
            emails = re.findall(r'[\w\.-]+@[\w\.-]+\.\w+', text)
            for email in emails:
                entity_id = self.sqlite_meta.upsert_entity(email, 'Person')
                if entity_id > 0:
                    self.sqlite_meta.add_entity_relation(entity_id, source_id, source_type, 'mentioned_in')

            tech_keywords = ['Python', 'Java', 'JavaScript', 'TypeScript', 'React', 'Vue', 'FastAPI', 
                           'Django', 'Spring', 'Docker', 'Kubernetes', 'AWS', 'Azure', 'GCP', 'SQL', 'NoSQL']
            for kw in tech_keywords:
                if kw.lower() in text.lower():
                    entity_id = self.sqlite_meta.upsert_entity(kw, 'Technology')
                    if entity_id > 0:
                        self.sqlite_meta.add_entity_relation(entity_id, source_id, source_type, 'mentioned_in')
        except Exception as e:
            self.logger.warning(f"ì—”í‹°í‹° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

    def _get_browser_paths(self) -> Dict[str, str]:
        """í˜„ì¬ ìš´ì˜ì²´ì œì— ë§ëŠ” ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ DB ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        system = platform.system()
        if system == 'Windows':
            return {
                'chrome': os.path.expanduser('~\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\History'),
                'edge': os.path.expanduser('~\\AppData\\Local\\Microsoft\\Edge\\User Data\\Default\\History')
            }
        elif system == 'Darwin':  # macOS
            return {
                'chrome': os.path.expanduser('~/Library/Application Support/Google/Chrome/Default/History'),
                'edge': os.path.expanduser('~/Library/Application Support/Microsoft Edge/Default/History')
                # Firefox, Safari ë“± ë‹¤ë¥¸ ë¸Œë¼ìš°ì € ì§€ì› ì¶”ê°€ ê°€ëŠ¥
            }
        # TODO: Add Linux support
        return {}

    async def _crawl_and_extract_text(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        if not url.startswith(('http://', 'https://')): return None
        try:
            async with session.get(url, timeout=10) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'lxml')
                    for s in soup(['script', 'style', 'nav', 'footer', 'aside']): s.decompose()
                    return soup.get_text(separator='\n', strip=True)
        except: return None

    def _batch_index_web_pages(self, history_data: List[Dict[str, Any]], repo: Repository, embedder: 'BGEM3Embedder'):
        async def main():
            all_texts, all_metas = [], []
            connector = aiohttp.TCPConnector(limit=20)
            async with aiohttp.ClientSession(connector=connector) as session:
                tasks = [self._crawl_and_extract_text(session, item['url']) for item in history_data]
                crawled_contents = await asyncio.gather(*tasks)
                self.logger.info("ğŸ“„ ì´ %dê°œ URL ì¤‘ %dê°œ í¬ë¡¤ë§ ì„±ê³µ",
                                 len(history_data), len([c for c in crawled_contents if c]))
                for item, content in zip(history_data, crawled_contents):
                    if not content: continue
                    chunks = self.parser.chunk_text(content)
                    doc_id = f"web_{hashlib.md5(item['url'].encode()).hexdigest()}"
                    for i, chunk in enumerate(chunks):
                        all_texts.append(chunk)
                        all_metas.append({
                            'user_id': self.user_id,  # user_id í¬í•¨
                            'source': 'web', 
                            'url': item['url'], 
                            'title': item['title'], 
                            'doc_id': doc_id, 
                            'chunk_id': i, 
                            'timestamp': int(item['visit_time'].timestamp()), 
                            'snippet': chunk[:200],
                            'content': chunk
                        })
                        
                        # [New] ì²« ë²ˆì§¸ ì²­í¬ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
                        if i == 0:
                            self._extract_and_save_entities(chunk[:2000], item['url'], 'web')
            if all_texts:
                self.logger.info("ğŸ§  BGE-M3ë¡œ %dê°œ ì›¹ ì²­í¬ ì„ë² ë”© ìƒì„±...", len(all_texts))
                embeddings = embedder.encode_documents(all_texts, batch_size=64)
                dense_vectors, sparse_vectors = embeddings['dense_vecs'].tolist(), [embedder.convert_sparse_to_qdrant_format(lw) for lw in embeddings['lexical_weights']]
                if repo.qdrant.upsert_vectors(all_metas, dense_vectors, sparse_vectors):
                    self.logger.info("âœ… Qdrantì— ì›¹ ì²­í¬ %dê°œ ì¸ë±ì‹± ì™„ë£Œ", len(dense_vectors))
                else:
                    self.logger.error("âŒ Qdrant ì›¹ ì²­í¬ ì¸ë±ì‹± ì‹¤íŒ¨")
        asyncio.run(main())

    def _get_browser_history(self, browser_name: str, incremental: bool) -> List[Dict[str, Any]]:
        db_path = self.browser_paths.get(browser_name.lower())
        if not db_path or not os.path.exists(db_path): return []
        temp_path, history_data = f"{db_path}_temp", []
        try:
            shutil.copy2(db_path, temp_path)
            conn, query, params = sqlite3.connect(temp_path), "SELECT url, title, last_visit_time FROM urls", ()
            if incremental and (last_time := self.sqlite_meta.get_last_browser_collection_time(self.user_id, browser_name)):
                webkit_ts = int((last_time - datetime(1601, 1, 1)).total_seconds() * 1_000_000)
                query, params = f"{query} WHERE last_visit_time > ?", (webkit_ts,)
            query += " ORDER BY last_visit_time DESC LIMIT 100"
            for row in conn.cursor().execute(query, params).fetchall():
                visit_time = datetime(1601, 1, 1) + timedelta(microseconds=row[2])
                if not self.sqlite_meta.is_browser_history_duplicate(self.user_id, row[0], visit_time):
                    history_data.append({'user_id': self.user_id, 'browser_name': browser_name, 'url': row[0], 'title': row[1], 'visit_time': visit_time})
            conn.close()
        except Exception as e:
            self.logger.error("%s íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì˜¤ë¥˜: %s", browser_name, e, exc_info=True)
        finally:
            if os.path.exists(temp_path): os.remove(temp_path)
        return history_data

    def collect_all_browser_history(self, incremental: bool = True) -> List[Dict[str, Any]]:
        return self._get_browser_history('Chrome', incremental) + self._get_browser_history('Edge', incremental)

    def save_browser_history_to_db(self, history_data: List[Dict[str, Any]], repo: Repository, embedder: 'BGEM3Embedder') -> int:
        if not history_data or not repo: return 0
        saved_count = 0
        try:
            self.sqlite_meta.conn.execute("BEGIN TRANSACTION")
            for item in history_data:
                if self.sqlite_meta.insert_collected_browser_history(item): saved_count += 1
            self.sqlite_meta.conn.commit()
            self.logger.info("âœ… SQLite ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ ì €ì¥: %dê°œ", saved_count)
        except Exception as e:
            self.sqlite_meta.conn.rollback()
            self.logger.error("âŒ SQLite íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: %s", e, exc_info=True)
            return 0
        self._batch_index_web_pages(history_data, repo, embedder)
        return saved_count

# -----------------------------------------------------------------------------
# ActiveApplicationCollector
# -----------------------------------------------------------------------------
class ActiveApplicationCollector:
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.sqlite_meta = SQLiteMeta()
    def collect_active_applications(self) -> List[Dict[str, Any]]:
        active_apps = []
        for proc in psutil.process_iter(['name', 'exe', 'create_time']):
            try:
                if proc.info['exe'] and os.path.exists(proc.info['exe']):
                    active_apps.append({'user_id': self.user_id, 'app_name': proc.info['name'], 'app_path': proc.info['exe'], 'start_time': datetime.fromtimestamp(proc.info['create_time'])})
            except (psutil.NoSuchProcess, psutil.AccessDenied): continue
        return active_apps
    def save_active_apps_to_db(self, apps_data: List[Dict[str, Any]]) -> int:
        saved = 0
        for app in apps_data:
            if self.sqlite_meta.insert_collected_app(app): saved += 1
        return saved


# -----------------------------------------------------------------------------
# SessionProcessor
# -----------------------------------------------------------------------------
class SessionProcessor:
    """
    Raw logs (collected_apps, collected_browser_history)ë¥¼ ë¶„ì„í•˜ì—¬
    ì˜ë¯¸ ìˆëŠ” ActivitySessionìœ¼ë¡œ ê·¸ë£¹í™”í•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.sqlite_meta = SQLiteMeta()
        self.logger = logger.getChild(f"SessionProcessor[user={user_id}]")
        self.session_timeout = 300  # 5ë¶„ (ì´ˆ ë‹¨ìœ„)

    def process_sessions(self):
        """ë¯¸ì²˜ë¦¬ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ì„¸ì…˜ì„ ìƒì„±í•˜ê±°ë‚˜ ê¸°ì¡´ ì„¸ì…˜ì— ì—°ê²°"""
        try:
            # 1. ë¯¸ì²˜ë¦¬ ì•± ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
            unprocessed_apps = self.sqlite_meta.get_unprocessed_logs("collected_apps", self.user_id)
            # 2. ë¯¸ì²˜ë¦¬ ë¸Œë¼ìš°ì € ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
            unprocessed_web = self.sqlite_meta.get_unprocessed_logs("collected_browser_history", self.user_id)
            
            all_logs = []
            for log in unprocessed_apps:
                all_logs.append({
                    'type': 'app', 'data': log, 'time': log['recorded_at'], 'id': log['id']
                })
            for log in unprocessed_web:
                all_logs.append({
                    'type': 'web', 'data': log, 'time': log['recorded_at'], 'id': log['id']
                })
            
            if not all_logs: return

            # ì‹œê°„ìˆœ ì •ë ¬
            all_logs.sort(key=lambda x: x['time'])

            current_session_id = None
            last_time = 0
            
            # ê°€ì¥ ìµœê·¼ ì„¸ì…˜ í™•ì¸ (ì´ì–´ë¶™ì´ê¸° ìœ„í•´)
            # (ê°„ì†Œí™”ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œëŠ” í•­ìƒ ìƒˆë¡œìš´ ì„¸ì…˜ ë¡œì§ì„ íƒ€ê±°ë‚˜, 
            #  ë©”ëª¨ë¦¬ì— ìƒíƒœë¥¼ ìœ ì§€í•˜ì§€ ì•Šê³  DB ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨)
            
            # ê°„ë‹¨í•œ ë¡œì§:
            # ë¡œê·¸ë¥¼ ìˆœíšŒí•˜ë©° ì´ì „ ë¡œê·¸ì™€ 5ë¶„ ì´ìƒ ì°¨ì´ë‚˜ë©´ ìƒˆ ì„¸ì…˜ ì‹œì‘
            
            pending_updates = {'app': [], 'web': []}
            
            for i, log in enumerate(all_logs):
                log_time = log['time']
                
                if current_session_id is None:
                    # ì²« ë¡œê·¸ -> ìƒˆ ì„¸ì…˜ ìƒì„±
                    current_session_id = self._create_new_session(log)
                    last_time = log_time
                else:
                    # ì‹œê°„ ì°¨ì´ í™•ì¸
                    if log_time - last_time > self.session_timeout:
                        # ì„¸ì…˜ ì¢…ë£Œ ë° ìƒˆ ì„¸ì…˜ ì‹œì‘
                        self._close_session(current_session_id, last_time)
                        current_session_id = self._create_new_session(log)
                    else:
                        # ê¸°ì¡´ ì„¸ì…˜ ìœ ì§€ (ì—…ë°ì´íŠ¸ëŠ” ë‚˜ì¤‘ì— í•œ ë²ˆì— í•˜ê±°ë‚˜ í•„ìš”ì‹œ)
                        pass
                
                last_time = log_time
                
                # ì„¸ì…˜ ID í• ë‹¹ ëŒ€ê¸°ì—´ ì¶”ê°€
                if log['type'] == 'app':
                    pending_updates['app'].append(log['id'])
                else:
                    pending_updates['web'].append(log['id'])
                
                # ë°°ì¹˜ ì—…ë°ì´íŠ¸ (ë˜ëŠ” ì„¸ì…˜ì´ ë°”ë€Œì—ˆì„ ë•Œ)
                if i == len(all_logs) - 1 or (i < len(all_logs)-1 and all_logs[i+1]['time'] - log_time > self.session_timeout):
                     self._flush_updates(current_session_id, pending_updates)
                     self._close_session(current_session_id, last_time) # ë§ˆì§€ë§‰ ë¡œê·¸ ì‹œê°„ìœ¼ë¡œ ì„¸ì…˜ ì¢…ë£Œ ì—…ë°ì´íŠ¸
                     pending_updates = {'app': [], 'web': []}
                     current_session_id = None # ë¦¬ì…‹

        except Exception as e:
            self.logger.error(f"ì„¸ì…˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)

    def _create_new_session(self, first_log) -> int:
        """ìƒˆ ì„¸ì…˜ ìƒì„±"""
        dominant_app = first_log['data'].get('app_name') if first_log['type'] == 'app' else "Browser"
        return self.sqlite_meta.create_activity_session(
            user_id=self.user_id,
            start_time=first_log['time'],
            dominant_app=dominant_app,
            summary="New Activity Started" # ë‚˜ì¤‘ì— AIë¡œ ì—…ë°ì´íŠ¸
        )

    def _close_session(self, session_id, end_time):
        """ì„¸ì…˜ ì¢…ë£Œ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        self.sqlite_meta.update_activity_session(session_id, end_time=end_time)

    def _flush_updates(self, session_id, updates):
        """ë¡œê·¸ì— ì„¸ì…˜ ID ë§¤í•‘"""
        if updates['app']:
            self.sqlite_meta.link_logs_to_session("collected_apps", updates['app'], session_id)
        if updates['web']:
            self.sqlite_meta.link_logs_to_session("collected_browser_history", updates['web'], session_id)


# -----------------------------------------------------------------------------
# DataCollectionManager
# -----------------------------------------------------------------------------
class DataCollectionManager:
    def __init__(self, user_id: int, repository: Repository, embedder: 'BGEM3Embedder'):
        self.user_id = user_id
        self.logger = logger.getChild(f"DataCollectionManager[user={user_id}]")
        self.file_collector = FileCollector(user_id)
        self.browser_collector = BrowserHistoryCollector(user_id)
        self.app_collector = ActiveApplicationCollector(user_id)
        self.session_processor = SessionProcessor(user_id)  # Add SessionProcessor
        self.running, self.initial_collection_done = False, False
        self.progress, self.progress_message = 0.0, "ì´ˆê¸°í™” ì¤‘..."
        self.logger.info("RAG ì‹œìŠ¤í…œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
        try:
            self.repository = repository
            self.embedder = embedder
            self.document_parser = DocumentParser()
            self.logger.info("âœ… RAG ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
        except Exception as e:
            self.logger.error("âŒ RAG ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: %s", e, exc_info=True)
            self.repository = self.embedder = self.document_parser = None

    def start_collection(self, selected_folders: List[str]):
        if self.running:
            self.logger.debug("ë°ì´í„° ìˆ˜ì§‘ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ìš”ì²­ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.")
            return

        self.selected_folders = selected_folders
        self.running = True
        self.collection_thread = threading.Thread(target=self._collection_loop, daemon=True)
        self.collection_thread.start()
        folder_desc = "ì „ì²´ ì‚¬ìš©ì í´ë”" if selected_folders is None else f"{len(selected_folders)}ê°œ í´ë”"
        self.logger.info("ì‚¬ìš©ì %dì˜ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ëŒ€ìƒ: %s", self.user_id, folder_desc)
    
    def perform_initial_collection(self, selected_folders: List[str]):
        """ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.repository:
            self.progress_message = "ì˜¤ë¥˜: RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨"
            self.logger.error("Repositoryê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.initial_collection_done = False
            return

        # ì„ íƒëœ í´ë”ë¥¼ ë‚˜ì¤‘ì— ë°±ê·¸ë¼ìš´ë“œ ìˆ˜ì§‘ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥í•©ë‹ˆë‹¤.
        self.selected_folders = selected_folders
        folder_desc = "ì „ì²´ ì‚¬ìš©ì í´ë”" if selected_folders is None else f"{len(selected_folders)}ê°œ í´ë”"
        self.logger.info("ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤. ëŒ€ìƒ: %s", folder_desc)

        success = False
        progress_points = {
            "file_collection": 50.0,
            "browser_history": 65.0,
            "file_embedding": 85.0,
            "browser_embedding": 95.0,
            "complete": 100.0
        }
        try:
            self.progress_message = "ğŸ“ ì´ˆê¸° íŒŒì¼ ìˆ˜ì§‘ ì¤‘..."
            files = self.file_collector.collect_files_from_drive(
                False,
                self,
                self.selected_folders,
                (0.0, progress_points["file_collection"])
            )

            self.progress = max(self.progress, progress_points["file_collection"])
            self.progress_message = "ğŸŒ ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì¤‘..."
            history = self.browser_collector.collect_all_browser_history(False)
            self.logger.debug("ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ %dê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ", len(history))

            self.progress = max(self.progress, progress_points["browser_history"])
            self.progress_message = "ğŸ’¾ íŒŒì¼ ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘..."
            self.file_collector.save_files_to_db(files, self.repository, self.embedder, self.document_parser, manager=self)
            
            self.progress = max(self.progress, progress_points["file_embedding"])
            self.progress_message = "ğŸ’¾ ì›¹ ì½˜í…ì¸  ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘..."
            self.browser_collector.save_browser_history_to_db(history, self.repository, self.embedder)

            self.progress = max(self.progress, progress_points["browser_embedding"])
            self.progress = progress_points["complete"]
            self.progress_message = "ğŸ‰ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!"
            self.logger.info("ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            success = True

        except Exception as e:
            self.logger.error("âŒ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: %s", e, exc_info=True)
            self.progress_message = "ì˜¤ë¥˜ ë°œìƒ"
        finally:
            self.initial_collection_done = success
            if not success:
                self.logger.warning("ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì´í›„ ìš”ì²­ ì‹œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                # ì´ˆê¸° ìˆ˜ì§‘ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ ë°±ê·¸ë¼ìš´ë“œ ìˆ˜ì§‘ ìë™ ì‹œì‘
                self.logger.info("ë°±ê·¸ë¼ìš´ë“œ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
                self.start_collection(selected_folders)
    
    def stop_collection(self):
        self.running = False
        if hasattr(self, 'collection_thread') and self.collection_thread:
            self.collection_thread.join()
        self.logger.info("ì‚¬ìš©ì %dì˜ ë°ì´í„° ìˆ˜ì§‘ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.", self.user_id)
    
    def _collection_loop(self):
        intervals = {'file': 3600, 'browser': 1800, 'app': 300}
        last_run = {key: 0 for key in intervals}
        while self.running:
            if not self.repository: time.sleep(10); continue
            current_time = time.time()
            if current_time - last_run['file'] >= intervals['file']: self._collect_files(); last_run['file'] = current_time
            if current_time - last_run['browser'] >= intervals['browser']: self._collect_browser_history(); last_run['browser'] = current_time
            if current_time - last_run['app'] >= intervals['app']: self._collect_active_apps(); last_run['app'] = current_time
            
            # ì„¸ì…˜ ì²˜ë¦¬ (ì•± ìˆ˜ì§‘ ì£¼ê¸°ì™€ ë§ì¶”ê±°ë‚˜ ë³„ë„ë¡œ ì‹¤í–‰)
            self.session_processor.process_sessions()
            
            time.sleep(10)

    def _collect_files(self):
        files = self.file_collector.collect_files_from_drive(True, self, self.selected_folders)
        self.file_collector.save_files_to_db(files, self.repository, self.embedder, self.document_parser, manager=self)
    
    def _collect_browser_history(self):
        history = self.browser_collector.collect_all_browser_history(True)
        self.browser_collector.save_browser_history_to_db(history, self.repository, self.embedder)
    
    def _collect_active_apps(self):
        apps = self.app_collector.collect_active_applications()
        self.app_collector.save_active_apps_to_db(apps)
    
# -----------------------------------------------------------------------------
# ì „ì—­ ê´€ë¦¬ í•¨ìˆ˜
# -----------------------------------------------------------------------------
data_collection_managers = {}
def get_manager(user_id: int, repository: Repository, embedder: 'BGEM3Embedder') -> DataCollectionManager:
    if user_id not in data_collection_managers:
        data_collection_managers[user_id] = DataCollectionManager(
            user_id=user_id,
            repository=repository,
            embedder=embedder
        )
    return data_collection_managers[user_id]