#!/usr/bin/env python3
"""
Data Collector Module (Keyword-Centric Architecture)
- íŒŒì¼, ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ ë°ì´í„° ìˆ˜ì§‘
- KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ë° content_keywords í…Œì´ë¸” ì €ì¥
- ê°„ì†Œí™”ëœ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ìµœì í™”
"""
import os
import sys
import warnings
from pathlib import Path
import aiohttp
from bs4 import BeautifulSoup
import shutil
import time
import sqlite3
import threading
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
import logging

# PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ê´€ë ¨ ê²½ê³  ì–µì œ (pypdfium2 ë©”ëª¨ë¦¬ ì •ë¦¬ ê²½ê³ )
warnings.filterwarnings('ignore', message='.*Cannot close object.*library is destroyed.*')
# PyTorch CUDA ê²½ê³  ì–µì œ (Docling ëª¨ë¸ ë¡œë”© ì‹œ)
warnings.filterwarnings('ignore', message='.*Attempting to deserialize object on.*CUDA.*')
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import asyncio
import hashlib

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬(backend)ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
backend_dir = Path(__file__).parent.parent.absolute()
if str(backend_dir) not in sys.path:
    sys.path.insert(0, str(backend_dir))

from config.settings import settings
from .repository import Repository
from .sqlite import SQLite
from .document_parser import DocumentParser
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from agents.chatbot_agent.rag.models.bge_m3_embedder import BGEM3Embedder

logger = logging.getLogger(__name__)

# KeywordExtractor ì‹±ê¸€í†¤ (Lazy Loading)
_keyword_extractor = None
_keyword_extractor_lock = threading.Lock()

def get_keyword_extractor():
    """KeywordExtractor ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _keyword_extractor
    if _keyword_extractor is None:
        with _keyword_extractor_lock:
            if _keyword_extractor is None:
                try:
                    from utils.keyword_extractor import KeywordExtractor
                    _keyword_extractor = KeywordExtractor.get_instance()
                    logger.info("âœ… KeywordExtractor ì‹±ê¸€í†¤ ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ KeywordExtractor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    return None
    return _keyword_extractor


def init_worker_logging():
    """
    ProcessPoolExecutor ì›Œì»¤ì˜ ë¡œê¹…ì„ ì™„ì „íˆ ì–µì œí•˜ì—¬
    ë¶ˆí•„ìš”í•œ INFO ë¡œê·¸(ëª¨ë“ˆ ì´ˆê¸°í™” ë“±) ìŠ¤íŒ¸ì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.WARNING)
    
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.WARNING)
    console_handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s'))
    root_logger.addHandler(console_handler)
    
    for logger_name in ['backend.config.logging_config', '__main__']:
        logging.getLogger(logger_name).setLevel(logging.WARNING)


def extract_keywords_from_text(text: str, top_n: int = 10) -> List[Tuple[str, float]]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
        top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
    
    Returns:
        [(keyword, score), ...] ë¦¬ìŠ¤íŠ¸
    """
    extractor = get_keyword_extractor()
    if extractor is None:
        return []
    
    try:
        return extractor.extract(text, top_n=top_n)
    except Exception as e:
        logger.debug(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []


def create_snippet(text: str, max_length: int = 200) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ ìŠ¤ë‹ˆí«ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not text:
        return ""
    text = text.strip()
    if len(text) <= max_length:
        return text
    truncated = text[:max_length]
    last_space = truncated.rfind(' ')
    if last_space > max_length * 0.7:
        truncated = truncated[:last_space]
    return truncated.rstrip() + "..."


# -----------------------------------------------------------------------------
# FileCollector
# -----------------------------------------------------------------------------
class FileCollector:
    """ì‚¬ìš©ì ë“œë¼ì´ë¸Œì˜ íŒŒì¼ë“¤ì„ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.logger = logger.getChild(f"FileCollector[user={user_id}]")
        self.sqlite = SQLite()
        self.supported_extensions = {
            'document': ['.txt', '.doc', '.docx', '.pdf', '.md', '.rtf', '.odt', '.tex'],
            'spreadsheet': ['.xls', '.xlsx', '.csv', '.ods', '.tsv'],
            'presentation': ['.ppt', '.pptx', '.odp', '.key'],
            'code': ['.py', '.js', '.ts', '.jsx', '.tsx', '.html', '.css', '.scss', '.java', '.cpp', '.c', '.h', 
                     '.cs', '.php', '.rb', '.go', '.rs', '.swift', '.kt', '.r', '.m', '.sh', '.bat', '.ps1',
                     '.sql', '.json', '.xml', '.yaml', '.yml', '.toml', '.ini', '.cfg', '.conf'],
            'note': ['.note', '.notes', '.evernote', '.onenote'],
            'ebook': ['.epub', '.mobi', '.azw', '.azw3'],
        }
        self.allowed_extensions = {ext for exts in self.supported_extensions.values() for ext in exts}

    def _get_directory_size(self, path: str) -> int:
        """ì¬ê·€ì ìœ¼ë¡œ ë””ë ‰í† ë¦¬ì˜ ì „ì²´ í¬ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        total_size = 0
        try:
            for dirpath, dirnames, filenames in os.walk(path):
                for f in filenames:
                    fp = os.path.join(dirpath, f)
                    if not os.path.islink(fp):
                        try:
                            total_size += os.path.getsize(fp)
                        except (OSError, FileNotFoundError):
                            continue
        except PermissionError:
            return 0
        return total_size

    def _format_size(self, size_bytes: int) -> str:
        """ë°”ì´íŠ¸ë¥¼ ì½ê¸° ì¢‹ì€ í˜•íƒœ(KB, MB, GB)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if size_bytes == 0:
            return "(0 bytes)"
        power = 1024
        n = 0
        power_labels = {0: 'bytes', 1: 'KB', 2: 'MB', 3: 'GB', 4: 'TB'}
        while size_bytes >= power and n < len(power_labels) - 1:
            size_bytes /= power
            n += 1
        return f"({size_bytes:.1f} {power_labels[n]})"

    def get_file_category(self, file_path: str) -> str:
        ext = Path(file_path).suffix.lower()
        for category, extensions in self.supported_extensions.items():
            if ext in extensions: return category
        return 'other'

    def should_skip_directory(self, dir_path: str) -> bool:
        skip_patterns = ['Windows', 'Program Files', '$Recycle.Bin', '.git', 'node_modules', '__pycache__', 'AppData']
        return any(part in Path(dir_path).parts for part in skip_patterns)

    def _generate_doc_id(self, file_path: str) -> str:
        """íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ doc_id ìƒì„±"""
        return f"file_{hashlib.md5(file_path.encode()).hexdigest()}"

    def is_file_modified(self, file_path: str, last_modified: datetime) -> bool:
        stored_modified = self.sqlite.get_file_last_modified(self.user_id, file_path)
        return stored_modified is None or last_modified > stored_modified

    def is_file_already_indexed(self, file_path: str) -> bool:
        """íŒŒì¼ì´ ì´ë¯¸ ì¸ë±ì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        doc_id = self._generate_doc_id(file_path)
        return self.sqlite.is_file_exists(self.user_id, doc_id)

    def get_user_folders(self, calculate_size: bool = True) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ì í™ˆ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  í´ë”ë¥¼ ìŠ¤ìº”í•˜ê³  í¬ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
        folders = []
        base_path = os.path.join(os.path.expanduser("~"), "Desktop")
        
        try:
            with os.scandir(base_path) as it:
                for entry in it:
                    if entry.is_dir() and not entry.is_symlink() and not entry.name.startswith('.'):
                        if self.should_skip_directory(entry.path):
                            continue
                        try:
                            stat = entry.stat()
                            
                            if calculate_size:
                                dir_size = self._get_directory_size(entry.path)
                                size_formatted = self._format_size(dir_size) if dir_size is not None else "(í¬ê¸° ê³„ì‚° ì‹¤íŒ¨)"
                            else:
                                dir_size = None
                                size_formatted = "(í¬ê¸° ë¯¸ê³„ì‚°)"

                            folders.append({
                                'name': entry.name,
                                'path': entry.path,
                                'size_formatted': size_formatted,
                                'modified_date': datetime.fromtimestamp(stat.st_mtime)
                            })
                        except (OSError, PermissionError):
                            continue
        except Exception as e:
            self.logger.error("ì‚¬ìš©ì í´ë”ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", e, exc_info=True)

        return sorted(folders, key=lambda x: x['name'].lower())

    def collect_files_from_drive(
        self,
        incremental: bool,
        manager: 'DataCollectionManager',
        selected_folders: Optional[List[str]],
        progress_bounds: Tuple[float, float] = (0.0, 50.0)
    ) -> List[Dict[str, Any]]:
        paths_to_scan = []
        if selected_folders is None:
            default_folders = self.get_user_folders()
            paths_to_scan = [folder['path'] for folder in default_folders]
        else:
            paths_to_scan = selected_folders
        
        if not paths_to_scan: 
            self.logger.warning("âš ï¸ ìŠ¤ìº”í•  í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        collected_files = []
        total_scanned = 0
        skipped_by_extension = 0
        skipped_by_duplicate = 0
        
        progress_start, progress_end = progress_bounds
        progress_range = max(progress_end - progress_start, 0.0)

        if manager:
            manager.progress = progress_start

        total_paths = len(paths_to_scan)

        for i, folder_path in enumerate(paths_to_scan):
            normalized_path = os.path.normpath(folder_path)
            
            if manager and total_paths > 0: 
                manager.progress = progress_start + ((i + 1) / total_paths) * progress_range
                manager.progress_message = f"ğŸ“ ìŠ¤ìº” ì¤‘: {Path(normalized_path).name}"
            
            try:
                for root, dirs, files in os.walk(normalized_path):
                    dirs[:] = [d for d in dirs if not self.should_skip_directory(os.path.join(root, d))]
                    for file in files:
                        try:
                            total_scanned += 1
                            file_path = os.path.join(root, file)
                            file_ext = Path(file_path).suffix.lower()
                            
                            if file.startswith("~$"):
                                continue
                            
                            if file_ext not in self.allowed_extensions:
                                skipped_by_extension += 1
                                continue
                            
                            stat = os.stat(file_path)
                            modified_date = datetime.fromtimestamp(stat.st_mtime)
                            
                            if incremental and not self.is_file_modified(file_path, modified_date):
                                continue
                            
                            # ì´ë¯¸ ì¸ë±ì‹±ëœ íŒŒì¼ì€ ìŠ¤í‚µ
                            if self.is_file_already_indexed(file_path):
                                skipped_by_duplicate += 1
                                continue
                            
                            collected_files.append({
                                'user_id': self.user_id,
                                'file_path': file_path,
                                'file_category': self.get_file_category(file_path),
                                'modified_date': modified_date,
                            })
                        except (PermissionError, OSError, FileNotFoundError): continue
            except Exception as e: 
                self.logger.error("í´ë” ìŠ¤ìº” ì˜¤ë¥˜ %s: %s", normalized_path, e, exc_info=True)
        
        if manager:
            if total_paths > 0:
                manager.progress = progress_end
            else:
                manager.progress = progress_start
        
        self.logger.info("ğŸ“Š íŒŒì¼ ìˆ˜ì§‘ ê²°ê³¼ - ì´ ìŠ¤ìº”: %d, í™•ì¥ì ì œì™¸: %d, ì¤‘ë³µ ì œì™¸: %d, ì‹ ê·œ íŒŒì¼: %d",
                         total_scanned, skipped_by_extension, skipped_by_duplicate, len(collected_files))
        
        if len(collected_files) == 0 and total_scanned > 0:
            self.logger.warning("âš ï¸ ì§€ì›ë˜ëŠ” í™•ì¥ì ëª©ë¡: %s", ', '.join(sorted(self.allowed_extensions)))
        
        return collected_files

    def save_files_to_db(
        self,
        files: List[Dict[str, Any]],
        repo: Repository,
        embedder: 'BGEM3Embedder',
        parser: DocumentParser,
        manager: Optional['DataCollectionManager'] = None
    ) -> int:
        if not files:
            self.logger.warning("âš ï¸ ì €ì¥í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        if not repo:
            self.logger.error("âš ï¸ Repositoryê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 0
            
        saved_count, text_files = 0, []
        try:
            conn = self.sqlite.get_user_connection(self.user_id)
            conn.execute("BEGIN TRANSACTION")
            for file_info in files:
                if self.sqlite.insert_collected_file(file_info):
                    saved_count += 1
                    if file_info['file_category'] in ['document', 'spreadsheet', 'presentation', 'code', 'note']:
                        text_files.append(file_info)
            conn.commit()
            self.logger.info("âœ… SQLite íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥: %dê°œ, í…ìŠ¤íŠ¸ ì¸ë±ì‹± ëŒ€ìƒ: %dê°œ",
                             saved_count, len(text_files))
        except Exception as e: 
            conn = self.sqlite.get_user_connection(self.user_id)
            if conn:
                conn.rollback()
            self.logger.error("âŒ SQLite íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: %s", e, exc_info=True)
            return 0
        
        if text_files:
            self._batch_index_text_files(text_files, repo, embedder, parser, manager)
        else:
            self.logger.warning("âš ï¸ í…ìŠ¤íŠ¸ ì¸ë±ì‹± ëŒ€ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return saved_count

    @staticmethod
    def _parse_single_file(file_info: Dict[str, Any], parser_ref: Any, user_id: int):
        """(í—¬í¼ í•¨ìˆ˜) ë‹¨ì¼ íŒŒì¼ì„ íŒŒì‹±. ProcessPoolExecutorì—ì„œ ì‹¤í–‰ë¨."""
        try:
            parser = None
            if parser_ref is not None:
                if isinstance(parser_ref, type):
                    parser = parser_ref()
                else:
                    parser = parser_ref
            if parser is None or not hasattr(parser, "parse_and_chunk"):
                from .document_parser import DocumentParser as _DocumentParser
                parser = _DocumentParser()

            try:
                chunk_infos = parser.parse_and_chunk(file_info['file_path'])
            except RuntimeError as e:
                return None, file_info.get('file_name', Path(file_info['file_path']).name), f"Docling RuntimeError: {e}"
            except Exception as e:
                return None, file_info.get('file_name', Path(file_info['file_path']).name), f"Parsing Exception: {e}"
            if not chunk_infos:
                return None, file_info.get('file_name', Path(file_info['file_path']).name), "ì²­í¬ ì—†ìŒ"

            doc_id = f"file_{hashlib.md5(file_info['file_path'].encode()).hexdigest()}"

            texts = []
            metas = []
            full_text_for_keywords = []  # í‚¤ì›Œë“œ ì¶”ì¶œìš© ì „ì²´ í…ìŠ¤íŠ¸
            
            for chunk in chunk_infos:
                texts.append(chunk['text'])
                metas.append({
                    'user_id': user_id,
                    'source': 'file',
                    'path': file_info['file_path'],
                    'doc_id': doc_id,
                    'chunk_id': chunk['chunk_id'],
                    'snippet': chunk['snippet'],
                    'content': chunk['text']
                })
                full_text_for_keywords.append(chunk['text'])

            file_name = Path(file_info['file_path']).name
            # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ë°˜í™˜ (í‚¤ì›Œë“œ ì¶”ì¶œìš©)
            combined_text = '\n'.join(full_text_for_keywords)
            
            return (texts, metas, file_info['file_path'], len(chunk_infos), doc_id, combined_text), file_name, None
        except BaseException as e:
            return None, file_info.get('file_name', 'unknown'), f"Worker setup error: {e}"

    def _extract_and_save_file_keywords(
        self, 
        doc_id: str, 
        combined_text: str,
        file_path: str
    ):
        """íŒŒì¼ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  content_keywords í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤."""
        if not combined_text or len(combined_text.strip()) < 50:
            return
        
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ (top 10)
            keywords = extract_keywords_from_text(combined_text, top_n=10)
            
            if not keywords:
                self.logger.debug(f"í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ ì—†ìŒ: {file_path}")
                return
            
            # ìŠ¤ë‹ˆí« ìƒì„±
            snippet = create_snippet(combined_text, max_length=200)
            
            # content_keywords í…Œì´ë¸”ì— ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
            keyword_entries = []
            for keyword, score in keywords:
                keyword_entries.append({
                    'user_id': self.user_id,
                    'source_type': 'file',
                    'source_id': doc_id,
                    'keyword': keyword,
                    'original_text': snippet
                })
            
            # ì¼ê´„ ì‚½ì…
            if keyword_entries:
                inserted = self.sqlite.insert_content_keywords_batch(self.user_id, keyword_entries)
                if inserted > 0:
                    self.logger.debug(f"ğŸ”‘ íŒŒì¼ í‚¤ì›Œë“œ ì €ì¥: {Path(file_path).name} - {inserted}ê°œ")
                    
        except Exception as e:
            self.logger.warning(f"íŒŒì¼ í‚¤ì›Œë“œ ì¶”ì¶œ/ì €ì¥ ì˜¤ë¥˜ ({file_path}): {e}")

    def _process_and_upload_batch(
        self,
        repo: Repository,
        embedder: 'BGEM3Embedder',
        texts: List[str],
        metas: List[Dict[str, Any]],
        batch_size: int
    ):
        """ì²­í¬ ë°°ì¹˜ë¥¼ ë°›ì•„ ì„ë² ë”©í•˜ê³  Qdrantì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
        if not texts:
            return

        self.logger.info(
            "ğŸ§  ì²­í¬ %dê°œ ë°°ì¹˜ ì„ë² ë”© ë° ì—…ë¡œë“œ ì¤‘... (Embedding Batch Size: %d)",
            len(texts),
            batch_size
        )
        try:
            embeddings = embedder.encode_documents(texts, batch_size=batch_size)
            dense_vectors = embeddings['dense_vecs'].tolist()
            sparse_vectors = [
                embedder.convert_sparse_to_qdrant_format(lw)
                for lw in embeddings['lexical_weights']
            ]

            if repo.qdrant.upsert_vectors(metas, dense_vectors, sparse_vectors):
                self.logger.info("   ... âœ… Qdrant ì—…ë¡œë“œ ì„±ê³µ: %dê°œ", len(texts))
            else:
                self.logger.error("   ... âŒ Qdrant ì—…ë¡œë“œ ì‹¤íŒ¨")
        except Exception as e:
            self.logger.error("   ... âŒ ì„ë² ë”©/ì—…ë¡œë“œ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: %s", e, exc_info=True)

    def _batch_index_text_files(
        self,
        text_files: List[Dict[str, Any]],
        repo: Repository,
        embedder: 'BGEM3Embedder',
        parser: DocumentParser,
        manager: Optional['DataCollectionManager'] = None
    ):
        # ì¤‘ë³µ íŒŒì¼ ê²½ë¡œ ì œê±°
        seen_paths = set()
        unique_text_files: List[Dict[str, Any]] = []
        duplicate_count = 0

        for file_info in text_files:
            file_path = file_info.get('file_path')
            if not file_path:
                self.logger.debug("í…ìŠ¤íŠ¸ ì¸ë±ì‹± ëŒ€ìƒì—ì„œ file_pathê°€ ì—†ëŠ” í•­ëª©ì„ ê±´ë„ˆëœë‹ˆë‹¤: %s", file_info)
                continue

            normalized_path = os.path.normcase(os.path.abspath(file_path))
            if normalized_path in seen_paths:
                duplicate_count += 1
                continue

            seen_paths.add(normalized_path)
            unique_text_files.append(file_info)

        if duplicate_count:
            self.logger.debug("í…ìŠ¤íŠ¸ ì¸ë±ì‹± ëŒ€ìƒì—ì„œ ì¤‘ë³µ íŒŒì¼ %dê°œë¥¼ ì œì™¸í–ˆìŠµë‹ˆë‹¤.", duplicate_count)

        text_files = unique_text_files

        cpu_count = multiprocessing.cpu_count()
        
        # ë©”ëª¨ë¦¬ ìµœì í™”: ì„¤ì •ì—ì„œ max_parallel_workers ë¡œë“œ (ê¸°ë³¸ê°’ 2)
        max_parallel_workers = getattr(parser, 'max_parallel_workers', 2) if parser else 2
        
        self.logger.info(
            "ğŸ“ í…ìŠ¤íŠ¸ íŒŒì¼ ì¸ë±ì‹± ì‹œì‘ - íŒŒì¼ %dê°œ, ì‚¬ìš© ì›Œì»¤ %dê°œ (ë©”ëª¨ë¦¬ ìµœì í™”)",
            len(text_files),
            max_parallel_workers
        )
        
        if manager:
            manager.progress_message = f"ğŸ“„ íŒŒì¼ íŒŒì‹± ì¤‘... (ì´ {len(text_files)}ê°œ)"

        is_gpu_available = getattr(embedder, "device", "cpu") == "cuda"
        embedding_batch_size = 128 if is_gpu_available else 32
        cpu_micro_batch_threshold = 5000

        all_texts: List[str] = []
        all_metas: List[Dict[str, Any]] = []
        total_chunk_count = 0
        parsed_count = 0
        failed_count = 0
        
        # í‚¤ì›Œë“œ ì¶”ì¶œìš© ë°ì´í„° ìˆ˜ì§‘
        files_for_keywords: List[Tuple[str, str, str]] = []  # (doc_id, combined_text, file_path)

        # ë©”ëª¨ë¦¬ ìµœì í™”: ì›Œì»¤ ìˆ˜ ì œí•œ (ê¸°ì¡´ min(cpu_count, 8) â†’ max_parallel_workers)
        max_workers = max_parallel_workers
        parser_ref = parser.__class__ if parser is not None else DocumentParser

        self.logger.info("--- [1/3] íŒŒì¼ íŒŒì‹± ì‹œì‘ (ì›Œì»¤ %dê°œ, ë©”ëª¨ë¦¬ ìµœì í™”) ---", max_workers)
        with ProcessPoolExecutor(max_workers=max_workers, initializer=init_worker_logging) as executor:
            futures = {
                executor.submit(self._parse_single_file, file_info, parser_ref, self.user_id): file_info
                for file_info in text_files
            }

            total_files = len(text_files)
            completed_files = 0
            
            for future in as_completed(futures):
                result, file_name, error = future.result()
                completed_files += 1
                
                if result:
                    texts, metas, file_path, chunk_count, doc_id, combined_text = result

                    all_texts.extend(texts)
                    all_metas.extend(metas)

                    total_chunk_count += len(texts)
                    parsed_count += 1
                    
                    # í‚¤ì›Œë“œ ì¶”ì¶œìš© ë°ì´í„° ì €ì¥
                    if combined_text:
                        files_for_keywords.append((doc_id, combined_text, file_path))
                    
                    self.logger.info("   âœ“ %s: %dê°œ ì²­í¬ (íŒŒì‹± ì™„ë£Œ)", file_name, chunk_count)
                else:
                    failed_count += 1
                    self.logger.warning("   âœ— íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜ %s: %s", file_name, error)
                
                if manager and total_files > 0:
                    manager.progress_message = f"ğŸ“„ íŒŒì¼ íŒŒì‹± ì¤‘... ({completed_files}/{total_files})"

        self.logger.info(
            "ğŸ“Š íŒŒì‹± ê²°ê³¼ - ì„±ê³µ %dê°œ, ì‹¤íŒ¨ %dê°œ, ì´ ì²­í¬ %dê°œ",
            parsed_count,
            failed_count,
            total_chunk_count
        )

        if not all_texts:
            self.logger.warning("âš ï¸ ì¸ë±ì‹±í•  í…ìŠ¤íŠ¸ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # --- [2/3] í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì €ì¥ ---
        self.logger.info("--- [2/3] í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì €ì¥ ì‹œì‘ ---")
        if manager:
            manager.progress_message = f"ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘... (ì´ {len(files_for_keywords)}ê°œ íŒŒì¼)"
        
        keyword_count = 0
        for doc_id, combined_text, file_path in files_for_keywords:
            self._extract_and_save_file_keywords(doc_id, combined_text, file_path)
            keyword_count += 1
            if manager and keyword_count % 10 == 0:
                manager.progress_message = f"ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘... ({keyword_count}/{len(files_for_keywords)})"
        
        self.logger.info("âœ… íŒŒì¼ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: %dê°œ íŒŒì¼", keyword_count)

        # --- [3/3] ì„ë² ë”© ë° ì—…ë¡œë“œ ---
        self.logger.info("--- [3/3] ì„ë² ë”© ë° ì—…ë¡œë“œ ì‹œì‘ (ëª¨ë“œ: %s) ---", "GPU" if is_gpu_available else "CPU")
        
        if manager:
            manager.progress_message = f"ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘... (ì´ {len(all_texts)}ê°œ ì²­í¬)"

        if is_gpu_available:
            if all_texts:
                self.logger.info("--- GPU ëª¨ë“œ: ì´ %dê°œ ì²­í¬ ì¼ê´„ ì²˜ë¦¬ ---", len(all_texts))
                self._process_and_upload_batch(
                    repo,
                    embedder,
                    all_texts,
                    all_metas,
                    embedding_batch_size
                )
        else:
            self.logger.warning("--- CPU ëª¨ë“œ: %dê°œ ì²­í¬ë¥¼ %dê°œ ë‹¨ìœ„ë¡œ ë¶„í•  ì²˜ë¦¬ ---",
                               len(all_texts), cpu_micro_batch_threshold)

            total_batches = (len(all_texts) + cpu_micro_batch_threshold - 1) // cpu_micro_batch_threshold
            
            for i in range(0, len(all_texts), cpu_micro_batch_threshold):
                batch_texts = all_texts[i:i + cpu_micro_batch_threshold]
                batch_metas = all_metas[i:i + cpu_micro_batch_threshold]

                if batch_texts:
                    batch_num = i // cpu_micro_batch_threshold + 1
                    self.logger.info(f"--- CPU ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ---")
                    
                    if manager:
                        manager.progress_message = f"ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘... (ë°°ì¹˜ {batch_num}/{total_batches})"
                    
                    self._process_and_upload_batch(
                        repo,
                        embedder,
                        batch_texts,
                        batch_metas,
                        embedding_batch_size
                    )

        self.logger.info("âœ… íŒŒì¼ ì¸ë±ì‹± ì™„ë£Œ: %dê°œ íŒŒì¼", parsed_count)


# -----------------------------------------------------------------------------
# BrowserHistoryCollector
# -----------------------------------------------------------------------------

# ë™ì‹œ ìš”ì²­ ì œí•œ (Rate Limiting) - DoS ì˜¤í•´ ë° IP ì°¨ë‹¨ ë°©ì§€
MAX_CONCURRENT_REQUESTS = 10
REQUEST_DELAY_SECONDS = 0.1  # ìš”ì²­ ê°„ ìµœì†Œ ë”œë ˆì´


class BrowserHistoryCollector:
    def __init__(self, user_id: int):
        self.user_id = user_id
        self.logger = logger.getChild(f"BrowserHistoryCollector[user={user_id}]")
        self.sqlite = SQLite()
        self.browser_paths = self._get_browser_paths()
        self.parser = DocumentParser()
        # ì„¸ë§ˆí¬ì–´: ë™ì‹œì— MAX_CONCURRENT_REQUESTSê°œê¹Œì§€ë§Œ ìš”ì²­ í—ˆìš©
        self._semaphore: Optional[asyncio.Semaphore] = None

    def _get_browser_paths(self) -> Dict[str, str]:
        """í˜„ì¬ ìš´ì˜ì²´ì œì— ë§ëŠ” ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ DB ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        import platform
        system = platform.system()
        if system == 'Windows':
            return {
                'chrome': os.path.expanduser('~\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\History'),
                'edge': os.path.expanduser('~\\AppData\\Local\\Microsoft\\Edge\\User Data\\Default\\History')
            }
        elif system == 'Darwin':  # macOS
            return {
                'chrome': os.path.expanduser('~/Library/Application Support/Google/Chrome/Default/History'),
                'edge': os.path.expanduser('~/Library/Application Support/Microsoft Edge/Default/History')
            }
        return {}

    def _fetch_web_content(self, url: str) -> Optional[str]:
        """URLì—ì„œ ë©”ì¸ ì½˜í…ì¸  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            from utils.web_crawler import fetch_web_content
            return fetch_web_content(url, timeout=3)
        except ImportError:
            self.logger.warning("web_crawler ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        except Exception as e:
            self.logger.debug(f"ì›¹ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return None

    def _extract_and_save_web_keywords(
        self,
        log_id: int,
        url: str,
        title: str,
        content: str
    ):
        """ì›¹ í˜ì´ì§€ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤."""
        if not content or len(content.strip()) < 50:
            return
        
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ (top 10)
            keywords = extract_keywords_from_text(content, top_n=10)
            
            if not keywords:
                return
            
            # ìŠ¤ë‹ˆí« ìƒì„±
            snippet = create_snippet(content, max_length=200)
            
            # content_keywords í…Œì´ë¸”ì— ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
            keyword_entries = []
            for keyword, score in keywords:
                keyword_entries.append({
                    'user_id': self.user_id,
                    'source_type': 'web',
                    'source_id': str(log_id),
                    'keyword': keyword,
                    'original_text': snippet
                })
            
            # ì¼ê´„ ì‚½ì…
            if keyword_entries:
                inserted = self.sqlite.insert_content_keywords_batch(self.user_id, keyword_entries)
                if inserted > 0:
                    self.logger.debug(f"ğŸ”‘ ì›¹ í‚¤ì›Œë“œ ì €ì¥: {title[:30]}... - {inserted}ê°œ")
                    
        except Exception as e:
            self.logger.warning(f"ì›¹ í‚¤ì›Œë“œ ì¶”ì¶œ/ì €ì¥ ì˜¤ë¥˜ ({url}): {e}")

    async def _crawl_with_rate_limit(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """
        ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ìš”ì²­ ìˆ˜ë¥¼ ì œí•œí•˜ë©´ì„œ URLì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        DoS ê³µê²©ìœ¼ë¡œ ì˜¤í•´ë°›ê±°ë‚˜ Rate Limitì— ê±¸ë¦¬ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
        """
        if self._semaphore is None:
            self._semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
        
        async with self._semaphore:
            # ìš”ì²­ ê°„ ìµœì†Œ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            await asyncio.sleep(REQUEST_DELAY_SECONDS)
            return await self._crawl_and_extract_text(session, url)
    
    async def _crawl_and_extract_text(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """
        ë¹„ë™ê¸°ë¡œ URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        SPA ì‚¬ì´íŠ¸ ë° ë™ì  ì½˜í…ì¸ ëŠ” ë‚´ìš©ì´ ë¹„ì–´ìˆê±°ë‚˜ ì§§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ
        100ì ë¯¸ë§Œì˜ ì½˜í…ì¸ ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
        """
        if not url.startswith(('http://', 'https://')): 
            return None
        
        url_lower = url.lower()
        
        # ìŠ¤í‚µí•  URL íŒ¨í„´ (SPA, ì†Œì…œë¯¸ë””ì–´, ì¸ì¦ í˜ì´ì§€ ë“±)
        skip_patterns = [
            # ì†Œì…œë¯¸ë””ì–´ (ëŒ€ë¶€ë¶„ SPA)
            'youtube.com', 'youtu.be', 'facebook.com', 'instagram.com', 
            'twitter.com', 'x.com', 'tiktok.com', 'linkedin.com/feed',
            'reddit.com', 'discord.com', 'slack.com', 'telegram.org',
            # ê²€ìƒ‰ ì—”ì§„
            'google.com/search', 'bing.com/search', 'naver.com/search',
            'duckduckgo.com', 'yahoo.com/search',
            # ì¸ì¦/ë¡œê·¸ì¸ í˜ì´ì§€
            'login', 'signin', 'signup', 'auth', 'oauth', 'sso',
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ìŠ¤íŠ¸ë¦¬ë°
            '.pdf', '.doc', '.zip', '.mp4', '.mp3', '.avi',
            'drive.google.com', 'dropbox.com', 'onedrive.live',
            # ì´ë©”ì¼
            'mail.google.com', 'outlook.live', 'mail.naver',
            # ê¸°íƒ€ SPA ì•±
            'notion.so', 'figma.com', 'canva.com', 'trello.com',
            'github.com/settings', 'gitlab.com/-/profile',
        ]
        if any(pattern in url_lower for pattern in skip_patterns):
            return None
        
        # íŒŒì¼ í™•ì¥ì ì²´í¬ (HTMLì´ ì•„ë‹Œ ë¦¬ì†ŒìŠ¤ ìŠ¤í‚µ)
        skip_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', 
                          '.css', '.js', '.json', '.xml', '.rss', '.ico']
        if any(url_lower.endswith(ext) for ext in skip_extensions):
            return None
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=5), headers=headers) as response:
                if response.status != 200:
                    return None
                
                # Content-Type í™•ì¸ (HTMLë§Œ ì²˜ë¦¬)
                content_type = response.headers.get('Content-Type', '')
                if 'text/html' not in content_type and 'application/xhtml' not in content_type:
                    return None
                
                html = await response.text()
                
                # HTMLì´ ë„ˆë¬´ ì§§ìœ¼ë©´ SPAì¼ ê°€ëŠ¥ì„± ë†’ìŒ
                if len(html) < 500:
                    return None
                
                # trafilatura ì‚¬ìš© ì‹œë„ (ë” ì •í™•í•œ ë³¸ë¬¸ ì¶”ì¶œ)
                try:
                    import trafilatura
                    extracted = trafilatura.extract(
                        html, 
                        include_comments=False, 
                        include_tables=False,
                        include_links=False,
                        include_images=False,
                        favor_recall=False  # ì •í™•ë„ ìš°ì„ 
                    )
                    if extracted and len(extracted.strip()) >= 100:  # 100ì ì´ìƒë§Œ ìœ íš¨
                        return extracted
                except ImportError:
                    pass
                
                # BeautifulSoup í´ë°±
                soup = BeautifulSoup(html, 'lxml')
                
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in soup(['script', 'style', 'nav', 'footer', 'aside', 
                                'header', 'noscript', 'iframe', 'form', 'button']): 
                    tag.decompose()
                
                # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„ (article, main, content ë“± ìš°ì„ )
                main_content = None
                for selector in ['article', 'main', '[role="main"]', '.content', '#content', '.post', '.article']:
                    main_content = soup.select_one(selector)
                    if main_content:
                        break
                
                if main_content:
                    text = main_content.get_text(separator='\n', strip=True)
                else:
                    text = soup.get_text(separator='\n', strip=True)
                
                # ìµœì†Œ 100ì ì´ìƒì¸ ê²½ìš°ë§Œ ë°˜í™˜ (SPA ì“°ë ˆê¸° ë°ì´í„° ë°©ì§€)
                if len(text.strip()) >= 100:
                    return text
                    
        except asyncio.TimeoutError:
            pass
        except Exception:
            pass
        return None

    def _batch_index_web_pages(self, history_data: List[Dict[str, Any]], repo: Repository, embedder: 'BGEM3Embedder'):
        """
        ì›¹ í˜ì´ì§€ë¥¼ ì¼ê´„ë¡œ í¬ë¡¤ë§í•˜ê³  ì¸ë±ì‹±í•©ë‹ˆë‹¤.
        
        ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ìš”ì²­ ìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤ (Rate Limiting).
        """
        async def main():
            all_texts, all_metas = [], []
            crawled_items = []  # í‚¤ì›Œë“œ ì¶”ì¶œìš©
            
            # ì„¸ë§ˆí¬ì–´ ì´ˆê¸°í™”
            self._semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
            
            # TCP ì—°ê²° ìˆ˜ë„ ì œí•œ
            connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT_REQUESTS)
            async with aiohttp.ClientSession(connector=connector) as session:
                # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
                tasks = [self._crawl_with_rate_limit(session, item['url']) for item in history_data]
                crawled_contents = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ì˜ˆì™¸ ì²˜ë¦¬: ì‹¤íŒ¨í•œ ìš”ì²­ì€ Noneìœ¼ë¡œ ëŒ€ì²´
                crawled_contents = [
                    None if isinstance(c, Exception) else c 
                    for c in crawled_contents
                ]
                
                success_count = len([c for c in crawled_contents if c])
                self.logger.info("ğŸ“„ ì´ %dê°œ URL ì¤‘ %dê°œ í¬ë¡¤ë§ ì„±ê³µ", len(history_data), success_count)
                
                for item, content in zip(history_data, crawled_contents):
                    if not content: 
                        continue
                    
                    # í‚¤ì›Œë“œ ì¶”ì¶œìš© ë°ì´í„° ì €ì¥
                    crawled_items.append({
                        'item': item,
                        'content': content
                    })
                    
                    chunks = self.parser.chunk_text(content)
                    doc_id = f"web_{hashlib.md5(item['url'].encode()).hexdigest()}"
                    for i, chunk in enumerate(chunks):
                        all_texts.append(chunk)
                        all_metas.append({
                            'user_id': self.user_id,
                            'source': 'web', 
                            'url': item['url'], 
                            'title': item['title'], 
                            'doc_id': doc_id, 
                            'chunk_id': i, 
                            'timestamp': int(item['visit_time'].timestamp()), 
                            'snippet': chunk[:200],
                            'content': chunk
                        })
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì €ì¥
            if crawled_items:
                self.logger.info("ğŸ”‘ ì›¹ í˜ì´ì§€ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘... (%dê°œ)", len(crawled_items))
                for crawled in crawled_items:
                    item = crawled['item']
                    content = crawled['content']
                    log_id = item.get('log_id', 0)
                    if log_id:
                        self._extract_and_save_web_keywords(
                            log_id=log_id,
                            url=item['url'],
                            title=item.get('title', ''),
                            content=content
                        )
                self.logger.info("âœ… ì›¹ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
            
            # Qdrant ì¸ë±ì‹±
            if all_texts:
                self.logger.info("ğŸ§  BGE-M3ë¡œ %dê°œ ì›¹ ì²­í¬ ì„ë² ë”© ìƒì„±...", len(all_texts))
                embeddings = embedder.encode_documents(all_texts, batch_size=64)
                dense_vectors = embeddings['dense_vecs'].tolist()
                sparse_vectors = [embedder.convert_sparse_to_qdrant_format(lw) for lw in embeddings['lexical_weights']]
                if repo.qdrant.upsert_vectors(all_metas, dense_vectors, sparse_vectors):
                    self.logger.info("âœ… Qdrantì— ì›¹ ì²­í¬ %dê°œ ì¸ë±ì‹± ì™„ë£Œ", len(dense_vectors))
                else:
                    self.logger.error("âŒ Qdrant ì›¹ ì²­í¬ ì¸ë±ì‹± ì‹¤íŒ¨")
        
        asyncio.run(main())

    def _get_browser_history(self, browser_name: str, incremental: bool) -> List[Dict[str, Any]]:
        db_path = self.browser_paths.get(browser_name.lower())
        if not db_path or not os.path.exists(db_path): return []
        temp_path, history_data = f"{db_path}_temp", []
        try:
            shutil.copy2(db_path, temp_path)
            conn, query, params = sqlite3.connect(temp_path), "SELECT url, title, last_visit_time FROM urls", ()
            if incremental and (last_time := self.sqlite.get_last_browser_collection_time(self.user_id, browser_name)):
                webkit_ts = int((last_time - datetime(1601, 1, 1)).total_seconds() * 1_000_000)
                query, params = f"{query} WHERE last_visit_time > ?", (webkit_ts,)
            query += " ORDER BY last_visit_time DESC LIMIT 100"
            for row in conn.cursor().execute(query, params).fetchall():
                visit_time = datetime(1601, 1, 1) + timedelta(microseconds=row[2])
                if not self.sqlite.is_browser_log_duplicate(self.user_id, row[0], visit_time):
                    history_data.append({
                        'user_id': self.user_id, 
                        'browser_name': browser_name, 
                        'url': row[0], 
                        'title': row[1], 
                        'visit_time': visit_time
                    })
            conn.close()
        except Exception as e:
            self.logger.error("%s íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì˜¤ë¥˜: %s", browser_name, e, exc_info=True)
        finally:
            if os.path.exists(temp_path): os.remove(temp_path)
        return history_data

    def collect_all_browser_history(self, incremental: bool = True) -> List[Dict[str, Any]]:
        return self._get_browser_history('Chrome', incremental) + self._get_browser_history('Edge', incremental)

    def save_browser_history_to_db(self, history_data: List[Dict[str, Any]], repo: Repository, embedder: 'BGEM3Embedder') -> int:
        if not history_data or not repo: 
            return 0
        
        saved_count = 0
        saved_items = []  # ì €ì¥ëœ í•­ëª© (log_id í¬í•¨)
        
        try:
            conn = self.sqlite.get_user_connection(self.user_id)
            conn.execute("BEGIN TRANSACTION")
            for item in history_data:
                log_id = self.sqlite.insert_collected_browser_history(item)
                if log_id:
                    saved_count += 1
                    # log_idë¥¼ itemì— ì¶”ê°€
                    item['log_id'] = log_id
                    saved_items.append(item)
            conn.commit()
            self.logger.info("âœ… SQLite ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ ì €ì¥: %dê°œ", saved_count)
        except Exception as e:
            conn = self.sqlite.get_user_connection(self.user_id)
            if conn:
                conn.rollback()
            self.logger.error("âŒ SQLite íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: %s", e, exc_info=True)
            return 0
        
        # ì €ì¥ëœ í•­ëª©ë§Œ ì¸ë±ì‹± (log_id í¬í•¨)
        if saved_items:
            self._batch_index_web_pages(saved_items, repo, embedder)
        
        return saved_count


# -----------------------------------------------------------------------------
# DataCollectionManager
# -----------------------------------------------------------------------------
class DataCollectionManager:
    """ë°ì´í„° ìˆ˜ì§‘ ê´€ë¦¬ì (í‚¤ì›Œë“œ ì¶”ì¶œ í¬í•¨)"""
    def __init__(self, user_id: int, repository: Repository, embedder: 'BGEM3Embedder'):
        self.user_id = user_id
        self.logger = logger.getChild(f"DataCollectionManager[user={user_id}]")
        self.file_collector = FileCollector(user_id)
        self.browser_collector = BrowserHistoryCollector(user_id)
        self.running, self.initial_collection_done = False, False
        self.progress, self.progress_message = 0.0, "ì´ˆê¸°í™” ì¤‘..."
        self.logger.info("RAG ì‹œìŠ¤í…œ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
        try:
            self.repository = repository
            self.embedder = embedder
            self.document_parser = DocumentParser()
            
            # KeywordExtractor ì‚¬ì „ ì´ˆê¸°í™” (Lazy Loadingì´ì§€ë§Œ ë¯¸ë¦¬ ì¤€ë¹„)
            get_keyword_extractor()
            
            self.logger.info("âœ… RAG ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
        except Exception as e:
            self.logger.error("âŒ RAG ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: %s", e, exc_info=True)
            self.repository = self.embedder = self.document_parser = None

    def start_collection(self, selected_folders: List[str]):
        if self.running:
            self.logger.debug("ë°ì´í„° ìˆ˜ì§‘ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ìš”ì²­ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.")
            return

        self.selected_folders = selected_folders
        self.running = True
        self.collection_thread = threading.Thread(target=self._collection_loop, daemon=True)
        self.collection_thread.start()
        folder_desc = "ì „ì²´ ì‚¬ìš©ì í´ë”" if selected_folders is None else f"{len(selected_folders)}ê°œ í´ë”"
        self.logger.info("ì‚¬ìš©ì %dì˜ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ëŒ€ìƒ: %s", self.user_id, folder_desc)
    
    def perform_initial_collection(self, selected_folders: List[str]):
        """ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.repository:
            self.progress_message = "ì˜¤ë¥˜: RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨"
            self.logger.error("Repositoryê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.initial_collection_done = False
            return

        self.selected_folders = selected_folders
        folder_desc = "ì „ì²´ ì‚¬ìš©ì í´ë”" if selected_folders is None else f"{len(selected_folders)}ê°œ í´ë”"
        self.logger.info("ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤. ëŒ€ìƒ: %s", folder_desc)

        success = False
        progress_points = {
            "file_collection": 50.0,
            "browser_history": 65.0,
            "file_embedding": 85.0,
            "browser_embedding": 95.0,
            "complete": 100.0
        }
        try:
            self.progress_message = "ğŸ“ ì´ˆê¸° íŒŒì¼ ìˆ˜ì§‘ ì¤‘..."
            files = self.file_collector.collect_files_from_drive(
                False,
                self,
                self.selected_folders,
                (0.0, progress_points["file_collection"])
            )

            self.progress = max(self.progress, progress_points["file_collection"])
            self.progress_message = "ğŸŒ ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì¤‘..."
            history = self.browser_collector.collect_all_browser_history(False)
            self.logger.debug("ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ %dê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ", len(history))

            self.progress = max(self.progress, progress_points["browser_history"])
            self.progress_message = "ğŸ’¾ íŒŒì¼ ì„ë² ë”© ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘..."
            self.file_collector.save_files_to_db(files, self.repository, self.embedder, self.document_parser, manager=self)
            
            self.progress = max(self.progress, progress_points["file_embedding"])
            self.progress_message = "ğŸ’¾ ì›¹ ì½˜í…ì¸  ì„ë² ë”© ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘..."
            self.browser_collector.save_browser_history_to_db(history, self.repository, self.embedder)

            self.progress = max(self.progress, progress_points["browser_embedding"])
            self.progress = progress_points["complete"]
            self.progress_message = "ğŸ‰ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!"
            self.logger.info("ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            success = True

        except Exception as e:
            self.logger.error("âŒ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: %s", e, exc_info=True)
            self.progress_message = "ì˜¤ë¥˜ ë°œìƒ"
        finally:
            self.initial_collection_done = success
            if not success:
                self.logger.warning("ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì´í›„ ìš”ì²­ ì‹œ ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                # ì§„í–‰ë¥  100% ìœ ì§€ (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì‹œ ë®ì–´ì“°ì§€ ì•Šë„ë¡)
                self.progress = 100.0
                self.progress_message = "âœ… ìˆ˜ì§‘ ì™„ë£Œ - ë°±ê·¸ë¼ìš´ë“œ ë™ê¸°í™” ì¤‘"
                
                # ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ í›„ ì¶”ì²œ ë¶„ì„ ì¦‰ì‹œ íŠ¸ë¦¬ê±°
                try:
                    from main import trigger_recommendation_analysis
                    asyncio.create_task(trigger_recommendation_analysis(force_recommend=True))
                    self.logger.info("ğŸ¯ ì´ˆê¸° ì¶”ì²œ ë¶„ì„ì´ íŠ¸ë¦¬ê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    self.logger.warning(f"ì¶”ì²œ ë¶„ì„ íŠ¸ë¦¬ê±° ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
                
                self.logger.info("ë°±ê·¸ë¼ìš´ë“œ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
                self.start_collection(selected_folders)
    
    def stop_collection(self):
        self.running = False
        if hasattr(self, 'collection_thread') and self.collection_thread:
            self.collection_thread.join()
        self.logger.info("ì‚¬ìš©ì %dì˜ ë°ì´í„° ìˆ˜ì§‘ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.", self.user_id)
    
    def _collection_loop(self):
        """ë°±ê·¸ë¼ìš´ë“œ ìˆ˜ì§‘ ë£¨í”„ (íŒŒì¼ ë° ë¸Œë¼ìš°ì €ë§Œ)"""
        intervals = {'file': 3600, 'browser': 1800}
        last_run = {key: 0 for key in intervals}
        while self.running:
            if not self.repository: 
                time.sleep(10)
                continue
            current_time = time.time()
            
            # ë°±ê·¸ë¼ìš´ë“œ ë™ê¸°í™” ì¤‘ì—ëŠ” ì§„í–‰ë¥ ì„ 100%ë¡œ ìœ ì§€
            if self.initial_collection_done:
                self.progress = 100.0
                self.progress_message = "âœ… ìˆ˜ì§‘ ì™„ë£Œ - ë°±ê·¸ë¼ìš´ë“œ ë™ê¸°í™” ì¤‘"
            
            if current_time - last_run['file'] >= intervals['file']: 
                self._collect_files()
                last_run['file'] = current_time
            if current_time - last_run['browser'] >= intervals['browser']: 
                self._collect_browser_history()
                last_run['browser'] = current_time
            time.sleep(10)

    def _collect_files(self):
        files = self.file_collector.collect_files_from_drive(True, self, self.selected_folders)
        self.file_collector.save_files_to_db(files, self.repository, self.embedder, self.document_parser, manager=self)
    
    def _collect_browser_history(self):
        history = self.browser_collector.collect_all_browser_history(True)
        self.browser_collector.save_browser_history_to_db(history, self.repository, self.embedder)

    
# -----------------------------------------------------------------------------
# ì „ì—­ ê´€ë¦¬ í•¨ìˆ˜
# -----------------------------------------------------------------------------
data_collection_managers = {}
def get_manager(user_id: int, repository: Repository, embedder: 'BGEM3Embedder') -> DataCollectionManager:
    if user_id not in data_collection_managers:
        data_collection_managers[user_id] = DataCollectionManager(
            user_id=user_id,
            repository=repository,
            embedder=embedder
        )
    return data_collection_managers[user_id]
