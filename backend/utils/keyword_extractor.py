"""
KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸° ëª¨ë“ˆ

Required packages:
    pip install keybert sentence-transformers

Uses 'paraphrase-multilingual-MiniLM-L12-v2' for Korean & English support.

Features:
    - ê¸´ í…ìŠ¤íŠ¸ ìë™ ì²­í‚¹ (BERT 512 í† í° ì œí•œ ëŒ€ì‘)
    - ì²­í¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ í›„ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
"""
import logging
from typing import List, Tuple, Optional
from collections import defaultdict
import threading

logger = logging.getLogger(__name__)

# BERT í† í° ì œí•œ ê´€ë ¨ ìƒìˆ˜
MAX_CHARS_PER_CHUNK = 2000  # ëŒ€ëµ 512 í† í°ì— í•´ë‹¹ (í•œê¸€ ê¸°ì¤€ ë³´ìˆ˜ì  ì¶”ì •)
CHUNK_OVERLAP_CHARS = 200  # ì²­í¬ ê°„ ì˜¤ë²„ë©

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë° ë½
_keyword_extractor_instance: Optional['KeywordExtractor'] = None
_keyword_extractor_lock = threading.Lock()


class KeywordExtractor:
    """
    KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸° (Singleton Pattern)
    
    í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ë‹¤êµ­ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ëª¨ë¸ ë¡œë”© ì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Usage:
        extractor = KeywordExtractor.get_instance()
        keywords = extractor.extract("í…ìŠ¤íŠ¸ ë‚´ìš©", top_n=10)
    """
    
    _MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"
    
    def __init__(self):
        """
        KeyBERT ëª¨ë¸ ì´ˆê¸°í™”
        
        Note: ì§ì ‘ ìƒì„±í•˜ì§€ ë§ê³  get_instance()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        """
        self._model = None
        self._initialized = False
        self._init_lock = threading.Lock()
    
    def _ensure_initialized(self):
        """ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ˆê¸°í™”í•©ë‹ˆë‹¤ (Lazy Loading)."""
        if self._initialized:
            return
        
        with self._init_lock:
            if self._initialized:
                return
            
            try:
                from keybert import KeyBERT
                
                logger.info(f"ğŸ”‘ KeyBERT ëª¨ë¸ ë¡œë”© ì¤‘... ({self._MODEL_NAME})")
                self._model = KeyBERT(model=self._MODEL_NAME)
                self._initialized = True
                logger.info("âœ… KeyBERT ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                
            except ImportError:
                logger.error(
                    "âŒ keybert íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                    "'pip install keybert sentence-transformers'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
                )
                raise
            except Exception as e:
                logger.error(f"âŒ KeyBERT ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
                raise
    
    @classmethod
    def get_instance(cls) -> 'KeywordExtractor':
        """
        ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            KeywordExtractor: ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
        """
        global _keyword_extractor_instance
        
        if _keyword_extractor_instance is None:
            with _keyword_extractor_lock:
                if _keyword_extractor_instance is None:
                    _keyword_extractor_instance = cls()
        
        return _keyword_extractor_instance
    
    def _chunk_text_for_bert(self, text: str) -> List[str]:
        """
        ê¸´ í…ìŠ¤íŠ¸ë¥¼ BERT ì…ë ¥ ì œí•œì— ë§ê²Œ ì²­í‚¹í•©ë‹ˆë‹¤.
        
        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if len(text) <= MAX_CHARS_PER_CHUNK:
            return [text]
        
        chunks = []
        separators = ["\n\n", "\n", ". ", " "]
        
        # ì¬ê·€ì  ë¶„í• 
        current_pos = 0
        while current_pos < len(text):
            end_pos = min(current_pos + MAX_CHARS_PER_CHUNK, len(text))
            
            if end_pos < len(text):
                # ê°€ì¥ ì ì ˆí•œ ë¶„í•  ì§€ì  ì°¾ê¸°
                best_split = end_pos
                for sep in separators:
                    # ì²­í¬ í¬ê¸° ë‚´ì—ì„œ ë§ˆì§€ë§‰ êµ¬ë¶„ì ìœ„ì¹˜ ì°¾ê¸°
                    search_start = max(current_pos, end_pos - 200)  # ë§ˆì§€ë§‰ 200ì ë‚´ì—ì„œ ê²€ìƒ‰
                    last_sep = text.rfind(sep, search_start, end_pos)
                    if last_sep > current_pos:
                        best_split = last_sep + len(sep)
                        break
                
                chunk = text[current_pos:best_split].strip()
                if chunk:
                    chunks.append(chunk)
                
                # ì˜¤ë²„ë© ì ìš©
                current_pos = max(current_pos + 1, best_split - CHUNK_OVERLAP_CHARS)
            else:
                chunk = text[current_pos:end_pos].strip()
                if chunk:
                    chunks.append(chunk)
                break
        
        return chunks
    
    def _merge_keywords(
        self, 
        all_keywords: List[List[Tuple[str, float]]], 
        top_n: int
    ) -> List[Tuple[str, float]]:
        """
        ì—¬ëŸ¬ ì²­í¬ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ë³‘í•©í•˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
        
        ê°™ì€ í‚¤ì›Œë“œê°€ ì—¬ëŸ¬ ì²­í¬ì—ì„œ ë“±ì¥í•˜ë©´ ì ìˆ˜ë¥¼ í•©ì‚°í•©ë‹ˆë‹¤.
        """
        keyword_scores = defaultdict(float)
        keyword_counts = defaultdict(int)
        
        for chunk_keywords in all_keywords:
            for keyword, score in chunk_keywords:
                keyword_lower = keyword.lower()
                keyword_scores[keyword_lower] += score
                keyword_counts[keyword_lower] += 1
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        merged = []
        for keyword, total_score in keyword_scores.items():
            count = keyword_counts[keyword]
            # ì—¬ëŸ¬ ì²­í¬ì—ì„œ ë“±ì¥í•œ í‚¤ì›Œë“œì— ì•½ê°„ì˜ ë³´ë„ˆìŠ¤
            avg_score = (total_score / count) * (1 + 0.1 * min(count - 1, 3))
            merged.append((keyword, round(avg_score, 4)))
        
        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ë°˜í™˜
        merged.sort(key=lambda x: x[1], reverse=True)
        return merged[:top_n]
    
    def extract(
        self,
        text: str,
        top_n: int = 10,
        keyphrase_ngram_range: Tuple[int, int] = (1, 2),
        stop_words: Optional[List[str]] = None,
        use_mmr: bool = True,
        diversity: float = 0.5
    ) -> List[Tuple[str, float]]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        ê¸´ í…ìŠ¤íŠ¸(ì•½ 2000ì ì´ˆê³¼)ëŠ” ìë™ìœ¼ë¡œ ì²­í‚¹í•˜ì—¬ ê° ì²­í¬ì—ì„œ 
        í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•œ í›„ ë³‘í•©í•©ë‹ˆë‹¤.
        
        Args:
            text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
            top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10)
            keyphrase_ngram_range: n-gram ë²”ìœ„ (ê¸°ë³¸ê°’: (1, 2) - ë‹¨ì–´ 1~2ê°œ)
            stop_words: ì œì™¸í•  ë¶ˆìš©ì–´ ëª©ë¡ (ê¸°ë³¸ê°’: None - ì˜ì–´ ë¶ˆìš©ì–´ ì‚¬ìš©)
            use_mmr: Maximal Marginal Relevance ì‚¬ìš© ì—¬ë¶€ (ë‹¤ì–‘ì„± í™•ë³´)
            diversity: MMR ë‹¤ì–‘ì„± ê³„ìˆ˜ (0~1, ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘)
        
        Returns:
            List[Tuple[str, float]]: (í‚¤ì›Œë“œ, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            
        Example:
            >>> extractor = KeywordExtractor.get_instance()
            >>> keywords = extractor.extract("Pythonì€ ë°ì´í„° ë¶„ì„ì— ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.", top_n=5)
            >>> print(keywords)
            [('Python', 0.82), ('ë°ì´í„° ë¶„ì„', 0.75), ...]
        """
        # ë¹ˆ í…ìŠ¤íŠ¸ ë˜ëŠ” ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if not text or len(text.strip()) < 10:
            logger.debug("í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ í‚¤ì›Œë“œ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return []
        
        self._ensure_initialized()
        
        try:
            # ê¸°ë³¸ ë¶ˆìš©ì–´ ì„¤ì • (ì˜ì–´ + í•œêµ­ì–´ ì¼ë¶€)
            if stop_words is None:
                stop_words = "english"
            
            # ê¸´ í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self._chunk_text_for_bert(text)
            
            if len(chunks) > 1:
                logger.debug(f"ê¸´ í…ìŠ¤íŠ¸ë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ")
            
            all_keywords = []
            
            for chunk in chunks:
                if len(chunk.strip()) < 10:
                    continue
                
                # KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ
                if use_mmr:
                    keywords = self._model.extract_keywords(
                        chunk,
                        keyphrase_ngram_range=keyphrase_ngram_range,
                        stop_words=stop_words,
                        top_n=top_n,
                        use_mmr=True,
                        diversity=diversity
                    )
                else:
                    keywords = self._model.extract_keywords(
                        chunk,
                        keyphrase_ngram_range=keyphrase_ngram_range,
                        stop_words=stop_words,
                        top_n=top_n
                    )
                
                # í›„ì²˜ë¦¬: ë¹ˆ í‚¤ì›Œë“œ ì œê±°
                processed = []
                for keyword, score in keywords:
                    keyword = keyword.strip()
                    if keyword and len(keyword) >= 2:
                        processed.append((keyword, round(score, 4)))
                
                if processed:
                    all_keywords.append(processed)
            
            # ì—¬ëŸ¬ ì²­í¬ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œ ë³‘í•©
            if len(all_keywords) > 1:
                return self._merge_keywords(all_keywords, top_n)
            elif len(all_keywords) == 1:
                return all_keywords[0][:top_n]
            else:
                return []
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return []
    
    def extract_simple(self, text: str, top_n: int = 10) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤ (ì ìˆ˜ ì œì™¸).
        
        Args:
            text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
            top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
        
        Returns:
            List[str]: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        keywords_with_scores = self.extract(text, top_n=top_n)
        return [kw for kw, _ in keywords_with_scores]
    
    def is_available(self) -> bool:
        """KeyBERT ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            self._ensure_initialized()
            return self._initialized and self._model is not None
        except Exception:
            return False


def get_keyword_extractor() -> KeywordExtractor:
    """
    KeywordExtractor ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜.
    
    Returns:
        KeywordExtractor: ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
    """
    return KeywordExtractor.get_instance()

